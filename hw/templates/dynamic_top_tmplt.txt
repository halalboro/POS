/**
 * This file is part of the Coyote <https://github.com/fpgasystems/Coyote>
 *
 * MIT Licence
 * Copyright (c) 2021-2025, Systems Group, ETH Zurich
 * All rights reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:

 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */

`timescale 1ns / 1ps
	
import lynxTypes::*;

`include "axi_macros.svh"
`include "lynx_macros.svh"
	
module design_dynamic_top #(
    parameter integer                           ID_DYN = 0
) (
    // AXI4 Lite control
    AXI4L.s                                     s_axi_ctrl [N_REGIONS],
    
{% if cnfg.en_avx %}
    // AXI4 AVX control
    AXI4.s                                      s_axim_ctrl [N_REGIONS],
        
{% endif %}
{% if cnfg.en_mem %}
    // AXI4 DDR 
    AXI4.m									    m_axi_ddr [1+N_REGIONS*N_CARD_AXI],
    
{% endif %}
{% if cnfg.en_strm %}
    // AXI4S host
    dmaIntf.m                                   m_host_dma_rd_req,
    dmaIntf.m                                   m_host_dma_wr_req,
    AXI4S.s                                     s_axis_host,
    AXI4S.m                                     m_axis_host,
        
{% endif %}
{% if cnfg.en_mem %}
    // AXI4S card
    dmaIntf.m                                   m_card_dma_rd_req,
    dmaIntf.m                                   m_card_dma_wr_req,
    AXI4S.s                                     s_axis_card,
    AXI4S.m                                     m_axis_card,
        
{% endif %}
{% if cnfg.en_net %}
    // ARP
    metaIntf.m                                  m_arp_lookup_request,

{% endif %}
{% if cnfg.en_rdma %}
    // RDMA
    metaIntf.m                                  m_rdma_qp_interface,
    metaIntf.m                                  m_rdma_conn_interface,
    metaIntf.m                                  m_rdma_sq,
    metaIntf.s                                  s_rdma_cq,
    metaIntf.s                                  s_rdma_rq_rd,
    metaIntf.s                                  s_rdma_rq_wr,
    AXI4S.s                                     s_axis_rdma,
    AXI4S.m                                     m_axis_rdma_req,
    AXI4S.m                                     m_axis_rdma_rsp,
    // POS: RDMA TX route_id for VIU VLAN tagging
    output logic [13:0]                         m_rdma_tx_route_id,

{% endif %}     
{% if cnfg.en_tcp %}
    // TCP/IP
    metaIntf.m                                  m_tcp_listen_req,
    metaIntf.s                                  s_tcp_listen_rsp,
    metaIntf.m                                  m_tcp_open_req,
    metaIntf.s                                  s_tcp_open_rsp,
    metaIntf.m                                  m_tcp_close_req,
    metaIntf.s                                  s_tcp_notify,
    metaIntf.m                                  m_tcp_rd_pkg,
    metaIntf.s                                  s_tcp_rx_meta,
    metaIntf.m                                  m_tcp_tx_meta,
    metaIntf.s                                  s_tcp_tx_stat,
    AXI4S.s                                     s_axis_tcp,
    AXI4S.m                                     m_axis_tcp,
    // POS: TCP route_id for VIU routing
    output logic [13:0]                         m_tcp_tx_route_id,
    output logic                                m_tcp_tx_route_id_valid,

{% endif %}
{% if cnfg.en_bypass %}
    // Bypass (raw Ethernet bypass path)
    metaIntf.m                                  m_bypass_sq,
    metaIntf.s                                  s_bypass_rq_rd,
    metaIntf.s                                  s_bypass_rq_wr,
    AXI4S.s                                     s_axis_bypass,
    AXI4S.m                                     m_axis_bypass,
    // POS: Bypass TX route_id for VIU VLAN tagging
    output logic [13:0]                         m_bypass_tx_route_id,

{% endif %}
{% if cnfg.en_wb %}
    // Writeback
    metaIntf.m                                  m_wback,

{% endif %}
    // Decoupling
    input  logic[N_REGIONS-1:0]                 s_decouple_sw,

    // IRQ
    output logic[N_REGIONS-1:0]                 m_usr_irq,

    // Debug
    input  logic [N_REGIONS-1:0]                S_BSCAN_drck,
    input  logic [N_REGIONS-1:0]                S_BSCAN_shift,
    input  logic [N_REGIONS-1:0]                S_BSCAN_tdi,
    input  logic [N_REGIONS-1:0]                S_BSCAN_update,
    input  logic [N_REGIONS-1:0]                S_BSCAN_sel,
    output logic [N_REGIONS-1:0]                S_BSCAN_tdo,
    input  logic [N_REGIONS-1:0]                S_BSCAN_tms,
    input  logic [N_REGIONS-1:0]                S_BSCAN_tck,
    input  logic [N_REGIONS-1:0]                S_BSCAN_runtest,
    input  logic [N_REGIONS-1:0]                S_BSCAN_reset,
    input  logic [N_REGIONS-1:0]                S_BSCAN_capture,
    input  logic [N_REGIONS-1:0]                S_BSCAN_bscanid_en,
    input  logic                                dclk,

    // Clock and reset
    input  logic                                aresetn,
    input  logic                                aclk,
    input  logic                                uresetn,
    input  logic                                uclk
);
	
    // ================-----------------------------------------------------------------
    // DECOUPLING 
    // ================-----------------------------------------------------------------

    // Decoupling signals
    logic [N_REGIONS-1:0] decouple;
    logic [N_REGIONS-1:0] decouple_uclk;
    
    dcpl_select inst_dcpl_select (
        .aclk(aclk),
        .aresetn(aresetn),

        .decouple_sw(s_decouple_sw), 
        .decouple(decouple)
    );

{% if cnfg.en_uclk %}  
    for(genvar i = 0; i < N_REGIONS; i++) begin
        xpm_cdc_single #(
            .DEST_SYNC_FF(4),  
            .INIT_SYNC_FF(0),  
            .SIM_ASSERT_CHK(0),
            .SRC_INPUT_REG(1)  
        ) (
            .dest_out(decouple_uclk[i]),
            .dest_clk(uclk),
            .src_clk(aclk),
            .src_in(decouple[i])
        );
    end
{% else %}
    assign decouple_uclk = decouple;
{% endif %}
	
{% if cnfg.en_strm %}
    // ================-----------------------------------------------------------------
    // HOST 
    // ================-----------------------------------------------------------------
    
    // XDMA host sync
    // ----------------------------------------------------------------------
    dmaIntf rd_XDMA_host();
    dmaIntf wr_XDMA_host();
    
    dma_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) inst_host_dma_reg_arr_s0_rd (.aclk(aclk), .aresetn(aresetn), .s_req(rd_XDMA_host), .m_req(m_host_dma_rd_req));
    dma_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) inst_host_dma_reg_arr_s0_wr (.aclk(aclk), .aresetn(aresetn), .s_req(wr_XDMA_host), .m_req(m_host_dma_wr_req));
    
    // Slice 0 
    // ----------------------------------------------------------------------
    AXI4S axis_host_in_s0();
    AXI4S axis_host_out_s0();

    axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) inst_host_axis_reg_arr_s0_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(s_axis_host),      .m_axis(axis_host_in_s0));
    axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) inst_host_axis_reg_arr_s0_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_host_out_s0), .m_axis(m_axis_host));

    // Multiplexing (via vIO Switch)
    // ----------------------------------------------------------------------
    // NOTE: With vIO Switch integration, the host data path is routed through
    // the vIO Switch instead of the direct axis_mux_host_src/sink muxes.
    // The vIO Switch routes host data based on route_id (vfid) attached as tdest.
    //
    // OLD PATH: DMA → axis_mux_host_src → axis_host_in_s1[i] → user
    // NEW PATH: DMA → vIO Switch HOST_TX → vFIU demux → axis_host_in_s1[i] → user
    //
    // The mux metadata interfaces are kept for compatibility but the data
    // routing is handled by vIO Switch.
    AXI4S axis_host_in_s1 [N_REGIONS] ();
    AXI4S axis_host_out_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(mux_host_t)) mux_host_rd_user ();
    metaIntf #(.STYPE(mux_host_t)) mux_host_wr_user ();

    // The old mux modules are commented out - routing is now via vIO Switch
    // axis_mux_host_src  inst_host_mux_in  (.aclk(aclk), .aresetn(aresetn), .s_mux(mux_host_rd_user), .s_axis(axis_host_in_s0),  .m_axis(axis_host_in_s1));
    // axis_mux_host_sink inst_host_mux_out (.aclk(aclk), .aresetn(aresetn), .s_mux(mux_host_wr_user), .s_axis(axis_host_out_s1), .m_axis(axis_host_out_s0));

    // mux_host_rd_user is consumed by dma_rd_route_sync (see vIO Switch section)
    // mux_host_wr_user is tied off - write metadata is handled separately
    assign mux_host_wr_user.ready = 1'b1;

    // Credits
    // ----------------------------------------------------------------------
    AXI4S axis_host_in_s2 [N_REGIONS] ();
    AXI4S axis_host_out_s2 [N_REGIONS] ();

{% if cnfg.en_cred_local == 0 %}
    logic [N_REGIONS-1:0] rxfer_host;
    logic [N_REGIONS-1:0] wxfer_host;

    for(genvar i = 0; i < N_REGIONS; i++) begin
        data_queue_credits_src  inst_host_dcred_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_host_in_s1[i]),  .m_axis(axis_host_in_s2[i]), .rxfer(rxfer_host[i]));
        data_queue_credits_sink inst_host_dcred_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_host_out_s2[i]), .m_axis(axis_host_out_s1[i]), .wxfer(wxfer_host[i]));
    end

{% else %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `AXIS_ASSIGN(axis_host_in_s1[i], axis_host_in_s2[i])
        `AXIS_ASSIGN(axis_host_out_s2[i], axis_host_out_s1[i])
    end

{% endif %}

    // Clock crossing (if enabled)
    // ----------------------------------------------------------------------
    AXI4S axis_host_in_s3 [N_REGIONS] ();
    AXI4S axis_host_out_s3 [N_REGIONS] ();

{% if cnfg.en_uclk %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        axis_ccross inst_host_ccross_in  (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_axis(axis_host_in_s2[i]),  .m_axis(axis_host_in_s3[i]));
        axis_ccross inst_host_ccross_out (.s_aclk(uclk), .s_aresetn(uresetn), .m_aclk(aclk), .m_aresetn(aresetn), .s_axis(axis_host_out_s3[i]), .m_axis(axis_host_out_s2[i]));
    end

{% else %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `AXIS_ASSIGN(axis_host_in_s2[i],  axis_host_in_s3[i])
        `AXIS_ASSIGN(axis_host_out_s3[i], axis_host_out_s2[i])
    end

{% endif %}	

    // Slice 1
    // ----------------------------------------------------------------------
    AXI4S axis_host_in_s4 [N_REGIONS] ();
    AXI4S axis_host_out_s4 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
{% if cnfg.en_uclk %}
        axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S1)) inst_host_axis_reg_arr_s1_in  (.aclk(uclk), .aresetn(uresetn), .s_axis(axis_host_in_s3[i]),  .m_axis(axis_host_in_s4[i]));
        axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S1)) inst_host_axis_reg_arr_s1_out (.aclk(uclk), .aresetn(uresetn), .s_axis(axis_host_out_s4[i]), .m_axis(axis_host_out_s3[i]));
{% else %}
        axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S1)) inst_host_axis_reg_arr_s1_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_host_in_s3[i]),  .m_axis(axis_host_in_s4[i]));
        axis_reg_array #(.N_STAGES(N_REG_DYN_HOST_S1)) inst_host_axis_reg_arr_s1_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_host_out_s4[i]), .m_axis(axis_host_out_s3[i]));
{% endif %}
    end

    // Decoupling
    // ----------------------------------------------------------------------
    AXI4S axis_host_in_ul [N_REGIONS] ();
    AXI4S axis_host_out_ul [N_REGIONS] ();

    axis_decoupler inst_host_dcpl_in  (.decouple(decouple_uclk), .s_axis(axis_host_in_s4),    .m_axis(axis_host_in_ul));
    axis_decoupler inst_host_dcpl_out (.decouple(decouple_uclk), .s_axis(axis_host_out_ul), .m_axis(axis_host_out_s4));

{% endif %}	
{% if cnfg.en_mem %}
    // ================-----------------------------------------------------------------
    // CARD 
    // ================-----------------------------------------------------------------

    AXI4 axi_ddr_s0[1+N_REGIONS*N_CARD_AXI] ();

    // XDMA card sync
    // ----------------------------------------------------------------------
    dmaIntf rd_XDMA_mig();
    dmaIntf wr_XDMA_mig();
    
    dma_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_dma_reg_arr_s0_sync_in  (.aclk(aclk), .aresetn(aresetn), .s_req(rd_XDMA_mig), .m_req(m_card_dma_rd_req));
    dma_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_dma_reg_arr_s0_sync_out (.aclk(aclk), .aresetn(aresetn), .s_req(wr_XDMA_mig), .m_req(m_card_dma_wr_req));
    
    // Slice init stage sync 
    // ----------------------------------------------------------------------
    AXI4S axis_card_sync_in_s0();
    AXI4S axis_card_sync_out_s0();

    axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_axis_reg_arr_s0_sync_in (.aclk(aclk), .aresetn(aresetn), .s_axis(s_axis_card),           .m_axis(axis_card_sync_in_s0));
    axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_axis_reg_arr_s0_sync_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_sync_out_s0), .m_axis(m_axis_card));
    
    // Memory sync 
    // ----------------------------------------------------------------------	
    dmaIntf rd_CDMA_mig();
    dmaIntf wr_CDMA_mig();

    cdma inst_cdma_sync (.aclk(aclk), .aresetn(aresetn),
        .rd_CDMA(rd_CDMA_mig), .wr_CDMA(wr_CDMA_mig), .s_axis_ddr(axis_card_sync_in_s0), .m_axis_ddr(axis_card_sync_out_s0), .m_axi_ddr(axi_ddr_s0[0]));
    
    axi_stripe #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_strp_sync (.aclk(aclk), .aresetn(aresetn), .s_axi(axi_ddr_s0[0]), .m_axi(m_axi_ddr[0]));

    // Slice init stage
    // ----------------------------------------------------------------------
    AXI4S axis_card_in_s0 [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_out_s0 [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_in_s1 [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_out_s1 [N_REGIONS*N_CARD_AXI] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_axis_reg_arr_s0_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_in_s0[i*N_CARD_AXI+j]),  .m_axis(axis_card_in_s1[i*N_CARD_AXI+j]));
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_card_axis_reg_arr_s0_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_out_s1[i*N_CARD_AXI+j]), .m_axis(axis_card_out_s0[i*N_CARD_AXI+j]));
        end
    end
    
    // Memory 
    // ----------------------------------------------------------------------	
    dmaIntf rd_CDMA_card [N_REGIONS*N_CARD_AXI] ();
    dmaIntf wr_CDMA_card [N_REGIONS*N_CARD_AXI] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            cdma inst_cdma (.aclk(aclk), .aresetn(aresetn), 
                .rd_CDMA(rd_CDMA_card[i*N_CARD_AXI+j]), .wr_CDMA(wr_CDMA_card[i*N_CARD_AXI+j]), .s_axis_ddr(axis_card_out_s0[i*N_CARD_AXI+j]), .m_axis_ddr(axis_card_in_s0[i*N_CARD_AXI+j]), .m_axi_ddr(axi_ddr_s0[i*N_CARD_AXI+j+1]));
        
            axi_stripe #(.N_STAGES(N_REG_DYN_CARD_S0)) inst_strp (.aclk(aclk), .aresetn(aresetn), .s_axi(axi_ddr_s0[i*N_CARD_AXI+j+1]), .m_axi(m_axi_ddr[i*N_CARD_AXI+j+1]));
        end
    end

    // Credits 
    // ----------------------------------------------------------------------
    AXI4S axis_card_in_s2 [N_REGIONS*N_CARD_AXI]();
    AXI4S axis_card_out_s2 [N_REGIONS*N_CARD_AXI] ();
{% if cnfg.en_mem_cred %}
    logic rxfer_card [N_REGIONS*N_CARD_AXI];
    logic wxfer_card [N_REGIONS*N_CARD_AXI];

    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            data_queue_credits_src  inst_card_dcred_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_in_s1[i*N_CARD_AXI+j]),  .m_axis(axis_card_in_s2[i*N_CARD_AXI+j]),  .rxfer(rxfer_card[i*N_CARD_AXI+j]));
            data_queue_credits_sink inst_card_dcred_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_out_s2[i*N_CARD_AXI+j]), .m_axis(axis_card_out_s1[i*N_CARD_AXI+j]), .wxfer(wxfer_card[i*N_CARD_AXI+j]));
        end
    end

{% else %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            `AXIS_ASSIGN(axis_card_in_s1[i*N_CARD_AXI+j], axis_card_in_s2[i*N_CARD_AXI+j])
            `AXIS_ASSIGN(axis_card_out_s2[i*N_CARD_AXI+j], axis_card_out_s1[i*N_CARD_AXI+j])
        end
    end

{% endif %}
    
    // Clock crossing (if enabled)
    // ----------------------------------------------------------------------
    AXI4S axis_card_in_s3 [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_out_s3 [N_REGIONS*N_CARD_AXI] ();

{% if cnfg.en_uclk %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            axis_ccross inst_card_ccross_in  (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_axis(axis_card_in_s2[i*N_CARD_AXI+j]),  .m_axis(axis_card_in_s3[i*N_CARD_AXI+j]));
            axis_ccross inst_card_ccross_out (.s_aclk(uclk), .s_aresetn(uresetn), .m_aclk(aclk), .m_aresetn(aresetn), .s_axis(axis_card_out_s3[i*N_CARD_AXI+j]), .m_axis(axis_card_out_s2[i*N_CARD_AXI+j]));
        end
    end

{% else %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
            `AXIS_ASSIGN(axis_card_in_s2[i*N_CARD_AXI+j],  axis_card_in_s3[i*N_CARD_AXI+j])
            `AXIS_ASSIGN(axis_card_out_s3[i*N_CARD_AXI+j], axis_card_out_s2[i*N_CARD_AXI+j])
        end
    end

{% endif %}	

    // Slice 1 
    // ----------------------------------------------------------------------
    AXI4S axis_card_in_s4 [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_out_s4 [N_REGIONS*N_CARD_AXI] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        for(genvar j = 0; j < N_CARD_AXI; j++) begin
{% if cnfg.en_uclk %}
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S1)) inst_card_axis_reg_arr_s1_in  (.aclk(uclk), .aresetn(uresetn), .s_axis(axis_card_in_s3[i*N_CARD_AXI+j]),  .m_axis(axis_card_in_s4[i*N_CARD_AXI+j]));
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S1)) inst_card_axis_reg_arr_s1_out (.aclk(uclk), .aresetn(uresetn), .s_axis(axis_card_out_s4[i*N_CARD_AXI+j]), .m_axis(axis_card_out_s3[i*N_CARD_AXI+j]));
{% else %}
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S1)) inst_card_axis_reg_arr_s1_in  (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_in_s3[i*N_CARD_AXI+j]),  .m_axis(axis_card_in_s4[i*N_CARD_AXI+j]));
            axis_reg_array #(.N_STAGES(N_REG_DYN_CARD_S1)) inst_card_axis_reg_arr_s1_out (.aclk(aclk), .aresetn(aresetn), .s_axis(axis_card_out_s4[i*N_CARD_AXI+j]), .m_axis(axis_card_out_s3[i*N_CARD_AXI+j]));
{% endif %}
        end
    end

    // Decoupling 
    // ----------------------------------------------------------------------		
    AXI4S axis_card_in_ul [N_REGIONS*N_CARD_AXI] ();
    AXI4S axis_card_out_ul [N_REGIONS*N_CARD_AXI] ();
    axis_decoupler #(.N_ID(N_REGIONS*N_CARD_AXI)) inst_card_dcpl_in  (.decouple(decouple_uclk), .s_axis(axis_card_in_s4),  .m_axis(axis_card_in_ul));
    axis_decoupler #(.N_ID(N_REGIONS*N_CARD_AXI)) inst_card_dcpl_out (.decouple(decouple_uclk), .s_axis(axis_card_out_ul), .m_axis(axis_card_out_s4));
		
{% endif %}	
{% if cnfg.en_net %}
    // ================-----------------------------------------------------------------
    // ARP
    // ================-----------------------------------------------------------------
    metaIntf #(.STYPE(logic [ARP_LUP_REQ_BITS-1:0])) arp_lup_req_s0 ();

    meta_reg_array #(.DATA_BITS(ARP_LUP_REQ_BITS), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(arp_lup_req_s0), .m_meta(m_arp_lookup_request));

{% endif %}
{% if cnfg.en_rdma %}
    // ================-----------------------------------------------------------------
    // RDMA
    // ================-----------------------------------------------------------------

    // Slice 0
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(rdma_qp_ctx_t)) rdma_qp_interface_s0 ();
    metaIntf #(.STYPE(rdma_qp_conn_t)) rdma_conn_interface_s0 ();

    metaIntf #(.STYPE(dreq_t)) rdma_sq_s0  ();
    metaIntf #(.STYPE(ack_t)) rdma_cq_s0  ();
    metaIntf #(.STYPE(req_t)) rdma_rq_rd_s0 ();
    metaIntf #(.STYPE(req_t)) rdma_rq_wr_s0 ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_in_s0  ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_req_s0  ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_rsp_s0  ();

    rdma_slice_array_dyn #(
        .N_STAGES(N_REG_DYN_NET_S0)
    ) inst_rdma_slice_array_0 (
        .aclk(aclk),
        .aresetn(aresetn),

        .m_rdma_qp_interface_n(m_rdma_qp_interface),
        .m_rdma_conn_interface_n(m_rdma_conn_interface),

        .m_rdma_sq_n(m_rdma_sq),
        .s_rdma_cq_n(s_rdma_cq),
        .s_rdma_rq_rd_n(s_rdma_rq_rd),
        .s_rdma_rq_wr_n(s_rdma_rq_wr),
        .m_axis_rdma_rd_req_n(m_axis_rdma_req),
        .m_axis_rdma_rd_rsp_n(m_axis_rdma_rsp),
        .s_axis_rdma_wr_n(s_axis_rdma),


        .s_rdma_qp_interface_u(rdma_qp_interface_s0),
        .s_rdma_conn_interface_u(rdma_conn_interface_s0),

        .s_rdma_sq_u(rdma_sq_s0),
        .m_rdma_cq_u(rdma_cq_s0),
        .m_rdma_rq_rd_u(rdma_rq_rd_s0),
        .m_rdma_rq_wr_u(rdma_rq_wr_s0),
        .s_axis_rdma_rd_req_u(axis_rdma_out_req_s0),
        .s_axis_rdma_rd_rsp_u(axis_rdma_out_rsp_s0),
        .m_axis_rdma_wr_u(axis_rdma_in_s0)
    );

    // Arbitrate METADATA only (data routing via vIO Switch)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) rdma_sq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) rdma_cq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) rdma_host_cq_s1 ();
    metaIntf #(.STYPE(req_t)) rdma_rq_rd_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_wr_s1 [N_REGIONS] ();

    // vIO Switch routing signals from metadata arbiter (full route_id format)
    // Route format: [13:10]=reserved, [9:6]=sender_id, [5:2]=receiver_id, [1:0]=flags
    logic [13:0] rdma_route_id_rx;
    logic [13:0] rdma_route_id_tx;

    rdma_meta_only_arbiter inst_rdma_meta_arbiter (
        .aclk(aclk),
        .aresetn(aresetn),

        // Network side - metadata only
        .m_rdma_sq_net(rdma_sq_s0),
        .s_rdma_cq_net(rdma_cq_s0),
        .s_rdma_rq_rd_net(rdma_rq_rd_s0),
        .s_rdma_rq_wr_net(rdma_rq_wr_s0),

        // User side - metadata only (per region)
        .s_rdma_sq_user(rdma_sq_s1),
        .m_rdma_cq_user(rdma_cq_s1),
        .m_rdma_host_cq_user(rdma_host_cq_s1),
        .m_rdma_rq_rd_user(rdma_rq_rd_s1),
        .m_rdma_rq_wr_user(rdma_rq_wr_s1),

        // vIO Switch routing (full route_id with sender_id encoded)
        .route_id_rx(rdma_route_id_rx),
        .route_id_tx(rdma_route_id_tx)
    );

{% if cnfg.en_bypass %}
    // ============================================================================
    // BYPASS - Network Interface (s0 stage)
    // ============================================================================
    // Bypass has same architecture as RDMA: metadata arbitration + vIO Switch data routing
    // s0 stage connects to network_stack's bypass_stack
    // ----------------------------------------------------------------------

    // Slice 0 - Network side
    metaIntf #(.STYPE(dreq_t)) bypass_sq_s0 ();
    metaIntf #(.STYPE(req_t)) bypass_rq_rd_s0 ();
    metaIntf #(.STYPE(req_t)) bypass_rq_wr_s0 ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_in_s0 ();   // RX: write data from network
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_out_s0 ();  // TX: read response to network

    bypass_slice_array_dyn #(
        .N_STAGES(N_REG_DYN_NET_S0)
    ) inst_bypass_slice_array_0 (
        .aclk(aclk),
        .aresetn(aresetn),

        // Network side (to/from bypass_stack via module ports)
        .m_bypass_sq_n(m_bypass_sq),
        .s_bypass_rq_rd_n(s_bypass_rq_rd),
        .s_bypass_rq_wr_n(s_bypass_rq_wr),
        .s_axis_bypass_wr_n(s_axis_bypass),
        .m_axis_bypass_rd_rsp_n(m_axis_bypass),

        // User side (to/from arbiter)
        .s_bypass_sq_u(bypass_sq_s0),
        .m_bypass_rq_rd_u(bypass_rq_rd_s0),
        .m_bypass_rq_wr_u(bypass_rq_wr_s0),
        .s_axis_bypass_rd_rsp_u(axis_bypass_out_s0),
        .m_axis_bypass_wr_u(axis_bypass_in_s0)
    );

    // Arbitrate BYPASS METADATA only (data routing via vIO Switch)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) bypass_sq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_rd_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_wr_s1 [N_REGIONS] ();

    // vIO Switch routing signals from bypass metadata arbiter
    logic [13:0] bypass_route_id_rx;
    logic [13:0] bypass_route_id_tx;

    bypass_meta_only_arbiter inst_bypass_meta_arbiter (
        .aclk(aclk),
        .aresetn(aresetn),

        // Network side - metadata only (to/from bypass_slice_array_dyn)
        .m_bypass_sq_net(bypass_sq_s0),
        .s_bypass_rq_rd_net(bypass_rq_rd_s0),
        .s_bypass_rq_wr_net(bypass_rq_wr_s0),

        // User side - metadata only (per region, to ccross/slice/user)
        .s_bypass_sq_user(bypass_sq_s1),
        .m_bypass_rq_rd_user(bypass_rq_rd_s1),
        .m_bypass_rq_wr_user(bypass_rq_wr_s1),

        // vIO Switch routing (full route_id with sender_id encoded)
        .route_id_rx(bypass_route_id_rx),
        .route_id_tx(bypass_route_id_tx)
    );

    // Export bypass TX route_id to module port for VIU VLAN tagging
    assign m_bypass_tx_route_id = bypass_route_id_tx;
{% endif %}

    // ============================================================================
    // POS vIO Switch Data Routing
    // ============================================================================
    // Data paths route through vIO Switch (centralized crossbar based on tdest).
    // vIO Switch uses AXI4SR interfaces (with tid/tdest for routing).
    //
    // Data flow:
    //   RDMA RX (network->vFPGA): axis_rdma_in_s0 -> vIO Switch -> vFIU[i] -> user
    //   RDMA TX REQ (vFPGA->network): user -> vFIU[i] -> vIO Switch -> axis_rdma_out_req_s0
    //   RDMA TX RSP (vFPGA->network): user -> vFIU[i] -> vIO Switch -> axis_rdma_out_rsp_s0
    //   vFPGA-to-vFPGA: user[i] -> vFIU[i] -> vIO Switch -> vFIU[j] -> user[j]
    // ----------------------------------------------------------------------

{% if cnfg.en_rdma %}
    // Per-region RDMA data interfaces (output from vIO Switch to user logic)
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_in_s1 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_req_s1 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_rsp_s1 [N_REGIONS] ();
{% endif %}

{% if cnfg.en_bypass %}
    // Bypass per-region data interfaces (RX = write data from network, TX = read response to network)
    // Note: bypass_sq_s1, bypass_rq_rd_s1, bypass_rq_wr_s1 are declared above with bypass_meta_only_arbiter
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_in_s1 [N_REGIONS] ();   // RX: vIO Switch -> vFIU -> user
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_out_s1 [N_REGIONS] ();  // TX: user -> vFIU -> vIO Switch
{% endif %}

    // AXI4SR interfaces for vIO Switch - RDMA ports (always declared for vio_switch compatibility)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_rx_to_switch ();      // RDMA RX -> Switch
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_rx_from_switch ();    // Switch -> (unused, RDMA RX is input only)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_tx_req_to_switch ();  // (unused, TX REQ is output only)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_tx_req_from_switch ();// Switch -> RDMA TX REQ
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_tx_rsp_to_switch ();  // (unused, TX RSP is output only)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_rdma_tx_rsp_from_switch ();// Switch -> RDMA TX RSP

{% if not cnfg.en_rdma %}
    // RDMA disabled - tie off all RDMA vIO Switch interfaces
    assign axisr_rdma_rx_to_switch.tvalid = 1'b0;
    assign axisr_rdma_rx_to_switch.tdata  = '0;
    assign axisr_rdma_rx_to_switch.tkeep  = '0;
    assign axisr_rdma_rx_to_switch.tlast  = 1'b0;
    assign axisr_rdma_rx_to_switch.tid    = '0;
    assign axisr_rdma_rx_to_switch.tdest  = '0;
    assign axisr_rdma_rx_from_switch.tready = 1'b1;
    assign axisr_rdma_tx_req_to_switch.tvalid = 1'b0;
    assign axisr_rdma_tx_req_to_switch.tdata  = '0;
    assign axisr_rdma_tx_req_to_switch.tkeep  = '0;
    assign axisr_rdma_tx_req_to_switch.tlast  = 1'b0;
    assign axisr_rdma_tx_req_to_switch.tid    = '0;
    assign axisr_rdma_tx_req_to_switch.tdest  = '0;
    assign axisr_rdma_tx_req_from_switch.tready = 1'b1;
    assign axisr_rdma_tx_rsp_to_switch.tvalid = 1'b0;
    assign axisr_rdma_tx_rsp_to_switch.tdata  = '0;
    assign axisr_rdma_tx_rsp_to_switch.tkeep  = '0;
    assign axisr_rdma_tx_rsp_to_switch.tlast  = 1'b0;
    assign axisr_rdma_tx_rsp_to_switch.tid    = '0;
    assign axisr_rdma_tx_rsp_to_switch.tdest  = '0;
    assign axisr_rdma_tx_rsp_from_switch.tready = 1'b1;
{% endif %}

    // vFIU interfaces to vIO Switch (per region)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_vfiu_to_switch [N_REGIONS] ();   // vFIU -> Switch (user TX)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_vfiu_from_switch [N_REGIONS] (); // Switch -> vFIU (user RX)

    // Route control signals for vFIUs (from host configuration)
    logic [N_REGIONS-1:0][13:0] vfiu_route_ctrl;
    logic [N_REGIONS-1:0][13:0] vfiu_route_in;
    logic [N_REGIONS-1:0][13:0] vfiu_route_out;
    logic [N_REGIONS-1:0]       vfiu_route_valid;       // Incoming route validation result (gateway_recv)
    logic [N_REGIONS-1:0]       vfiu_route_bypass;      // Trusted path bypass indicator (gateway_recv)
    logic [N_REGIONS-1:0]       vfiu_send_route_valid;  // Outgoing route validation result (gateway_send)

    // Route extracted from RDMA TX path (vIO Switch output) for VIU VLAN tagging
    logic [13:0] rdma_tx_route_out;

    // Hardcoded route_ctrl for testing (will be replaced by host configuration)
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_route_ctrl
        assign vfiu_route_ctrl[i] = 14'h0000;  // Default: no restriction
    end

{% if cnfg.en_rdma %}
    // Convert AXI4S (network) to AXI4SR (vIO Switch) for RDMA RX
    // tdest = route_id_rx from metadata arbiter (includes sender_id=PORT_RDMA_RX)
    assign axisr_rdma_rx_to_switch.tvalid = axis_rdma_in_s0.tvalid;
    assign axisr_rdma_rx_to_switch.tdata  = axis_rdma_in_s0.tdata;
    assign axisr_rdma_rx_to_switch.tkeep  = axis_rdma_in_s0.tkeep;
    assign axisr_rdma_rx_to_switch.tlast  = axis_rdma_in_s0.tlast;
    assign axisr_rdma_rx_to_switch.tid    = '0;
    // tdest = full route_id with sender_id=PORT_RDMA_RX, receiver_id=vfid
    assign axisr_rdma_rx_to_switch.tdest  = rdma_route_id_rx;
    assign axis_rdma_in_s0.tready         = axisr_rdma_rx_to_switch.tready;

    // Convert AXI4SR (vIO Switch) to AXI4S (network) for RDMA TX REQ
    // Extract route from tdest for VIU VLAN tagging
    assign axis_rdma_out_req_s0.tvalid         = axisr_rdma_tx_req_from_switch.tvalid;
    assign axis_rdma_out_req_s0.tdata          = axisr_rdma_tx_req_from_switch.tdata;
    assign axis_rdma_out_req_s0.tkeep          = axisr_rdma_tx_req_from_switch.tkeep;
    assign axis_rdma_out_req_s0.tlast          = axisr_rdma_tx_req_from_switch.tlast;
    assign axisr_rdma_tx_req_from_switch.tready = axis_rdma_out_req_s0.tready;

    // Convert AXI4SR (vIO Switch) to AXI4S (network) for RDMA TX RSP
    assign axis_rdma_out_rsp_s0.tvalid         = axisr_rdma_tx_rsp_from_switch.tvalid;
    assign axis_rdma_out_rsp_s0.tdata          = axisr_rdma_tx_rsp_from_switch.tdata;
    assign axis_rdma_out_rsp_s0.tkeep          = axisr_rdma_tx_rsp_from_switch.tkeep;
    assign axis_rdma_out_rsp_s0.tlast          = axisr_rdma_tx_rsp_from_switch.tlast;
    assign axisr_rdma_tx_rsp_from_switch.tready = axis_rdma_out_rsp_s0.tready;

    // Extract route_id from TX path for VIU gateway_tx (VLAN tagging)
    // Priority: TX REQ over TX RSP (they shouldn't be simultaneous, but just in case)
    // This route_id is passed to VIU to encode in the VLAN tag for outgoing packets
    assign rdma_tx_route_out = axisr_rdma_tx_req_from_switch.tvalid ?
                               axisr_rdma_tx_req_from_switch.tdest :
                               axisr_rdma_tx_rsp_from_switch.tdest;

    // Export TX route to module port for VIU connection
    assign m_rdma_tx_route_id = rdma_tx_route_out;

    // Tie off unused sink interfaces (these ports only receive, not send)
    assign axisr_rdma_tx_req_to_switch.tvalid = 1'b0;
    assign axisr_rdma_tx_req_to_switch.tdata  = '0;
    assign axisr_rdma_tx_req_to_switch.tkeep  = '0;
    assign axisr_rdma_tx_req_to_switch.tlast  = 1'b0;
    assign axisr_rdma_tx_req_to_switch.tid    = '0;
    assign axisr_rdma_tx_req_to_switch.tdest  = '0;

    assign axisr_rdma_tx_rsp_to_switch.tvalid = 1'b0;
    assign axisr_rdma_tx_rsp_to_switch.tdata  = '0;
    assign axisr_rdma_tx_rsp_to_switch.tkeep  = '0;
    assign axisr_rdma_tx_rsp_to_switch.tlast  = 1'b0;
    assign axisr_rdma_tx_rsp_to_switch.tid    = '0;
    assign axisr_rdma_tx_rsp_to_switch.tdest  = '0;

    assign axisr_rdma_rx_from_switch.tready = 1'b1;  // Always accept (unused)
{% endif %}

    // ========================================================================================
    // Host vIO Switch Interfaces
    // ========================================================================================
    // HOST_TX: DMA read response data → vIO Switch → vFIU (to vFPGA)
    // HOST_RX: vFIU → vIO Switch → DMA write data (from vFPGA)
    //
    // The host data path is routed through the vIO Switch for proper isolation.
    // - HOST_TX receives DMA read responses with route_id attached as tdest
    // - HOST_RX sends vFPGA write data to DMA after vIO Switch routing
    //
    // Port assignments (from vio_switch):
    //   PORT_HOST_TX = N_REGIONS + 0
    //   PORT_HOST_RX = N_REGIONS + 1

    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_host_tx_to_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_host_tx_from_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_host_rx_to_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_host_rx_from_switch ();

    // Per-vFIU P2P (vFPGA-to-vFPGA) data interfaces
    // P2P path uses AXI4SR with TID (like microShell) - no decoupler/ccross in this path
    // RX: validated by gateway_recv in vfiu_top, TID carries sender vFPGA ID
    // TX: user sends with TID, gateway_send in vfiu_top validates and builds route_id
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_p2p_in [N_REGIONS] ();        // To user logic (tid = sender vFPGA)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_p2p_out [N_REGIONS] ();       // From user (with tid)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_p2p_in_ul [N_REGIONS] ();     // User level RX (direct, no pipeline)
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_p2p_out_ul [N_REGIONS] ();    // User level TX (direct, no pipeline)

    // HOST_TX: DMA read response → vIO Switch
    // Use dma_rd_route_sync to synchronize mux metadata (vfid) with data transfers.
    // The mux metadata comes from mmu_arbiter and tells us which vFPGA each data chunk belongs to.
    // Route format: [13:10]=reserved, [9:6]=sender_id (PORT_HOST_TX), [5:2]=receiver_id (vfid), [1:0]=flags
    dma_rd_route_sync #(
        .MUX_DATA_BITS(AXI_DATA_BITS),
        .N_REGIONS(N_REGIONS)
    ) inst_dma_rd_route_sync (
        .aclk(aclk),
        .aresetn(aresetn),
        .s_mux(mux_host_rd_user),
        .s_axis(axis_host_in_s0),
        .m_axis(axisr_host_tx_to_switch)
    );

    // HOST_TX src goes to vFIU demux (handled in gen_vfiu_connections)
    // axisr_host_tx_from_switch is connected to vFIU RX path

    // HOST_RX: vIO Switch → DMA write data
    // Connect HOST_RX src to axis_host_out_s0 (DMA write data input)
    assign axis_host_out_s0.tvalid        = axisr_host_rx_from_switch.tvalid;
    assign axis_host_out_s0.tdata         = axisr_host_rx_from_switch.tdata;
    assign axis_host_out_s0.tkeep         = axisr_host_rx_from_switch.tkeep;
    assign axis_host_out_s0.tlast         = axisr_host_rx_from_switch.tlast;
    assign axisr_host_rx_from_switch.tready = axis_host_out_s0.tready;

    // HOST_RX sink comes from vFIU mux (handled in gen_vfiu_connections)
    // axisr_host_rx_to_switch is connected to vFIU TX path

    // TCP vIO Switch interface
    // POS: TCP data flows bidirectionally through vIO Switch for routing
    // - RX (network->vFPGA): axis_tcp_in_s0 -> vIO Switch TCP sink -> vFIU -> user
    // - TX (vFPGA->network): user -> vFIU -> vIO Switch TCP src -> axis_tcp_out_s0
    //
    // Note: vIO Switch has a single bidirectional TCP port (sink/src).
    // Metadata is handled by tcp_meta_only_arbiter, data routing by vIO Switch.
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_tcp_to_switch ();   // RX data to vIO Switch
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_tcp_from_switch (); // TX data from vIO Switch

{% if not cnfg.en_tcp %}
    // TCP disabled - tie off TCP vIO Switch interfaces
    assign axisr_tcp_to_switch.tvalid = 1'b0;
    assign axisr_tcp_to_switch.tdata  = '0;
    assign axisr_tcp_to_switch.tkeep  = '0;
    assign axisr_tcp_to_switch.tlast  = 1'b0;
    assign axisr_tcp_to_switch.tid    = '0;
    assign axisr_tcp_to_switch.tdest  = '0;
    assign axisr_tcp_from_switch.tready = 1'b1;
{% endif %}

    // Bypass vIO Switch interfaces (always declared for vio_switch compatibility)
    // POS: Bypass data flows through vIO Switch for routing
    // - RX (network->vFPGA): bypass_stack -> vIO Switch -> vFIU -> user
    // - TX (vFPGA->network): user -> vFIU -> vIO Switch -> bypass_stack
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_rx_to_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_rx_from_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_tx_req_to_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_tx_req_from_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_tx_rsp_to_switch ();
    AXI4SR #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axisr_bypass_tx_rsp_from_switch ();

{% if cnfg.en_bypass %}
    // ============================================================================
    // Bypass RX Path: Network -> vIO Switch -> vFIU -> User
    // ============================================================================
    // Convert AXI4S (from bypass_stack via axis_bypass_in_s0) to AXI4SR (for vIO Switch)
    // tdest = bypass_route_id_rx from metadata arbiter (includes sender_id=PORT_BYPASS_RX)
    // Route format: [13:10]=reserved, [9:6]=sender_id, [5:2]=receiver_id, [1:0]=flags
    // For RX from network: sender=PORT_BYPASS_RX, receiver=vfid (destination vFPGA)
    assign axisr_bypass_rx_to_switch.tvalid = axis_bypass_in_s0.tvalid;
    assign axisr_bypass_rx_to_switch.tdata  = axis_bypass_in_s0.tdata;
    assign axisr_bypass_rx_to_switch.tkeep  = axis_bypass_in_s0.tkeep;
    assign axisr_bypass_rx_to_switch.tlast  = axis_bypass_in_s0.tlast;
    assign axisr_bypass_rx_to_switch.tid    = '0;
    // tdest = full route_id with sender_id=PORT_BYPASS_RX, receiver_id=vfid
    assign axisr_bypass_rx_to_switch.tdest  = bypass_route_id_rx;
    assign axis_bypass_in_s0.tready         = axisr_bypass_rx_to_switch.tready;

    // ============================================================================
    // Bypass TX Path: User -> vFIU -> vIO Switch -> Network
    // ============================================================================
    // Convert AXI4SR (from vIO Switch bypass TX src) to AXI4S (to bypass_stack via axis_bypass_out_s0)
    // Bypass TX uses the REQ port (like RDMA, TX RSP is unused for bypass)
    assign axis_bypass_out_s0.tvalid           = axisr_bypass_tx_req_from_switch.tvalid;
    assign axis_bypass_out_s0.tdata            = axisr_bypass_tx_req_from_switch.tdata;
    assign axis_bypass_out_s0.tkeep            = axisr_bypass_tx_req_from_switch.tkeep;
    assign axis_bypass_out_s0.tlast            = axisr_bypass_tx_req_from_switch.tlast;
    assign axisr_bypass_tx_req_from_switch.tready = axis_bypass_out_s0.tready;

    // Bypass TX REQ to switch: tie off (TX data comes from vFIU, not directly here)
    // The vFIU TX path routes through axisr_vfiu_to_switch, which goes to appropriate
    // destination based on tdest. For bypass TX, vFIU sets tdest to route to bypass port.
    assign axisr_bypass_tx_req_to_switch.tvalid = 1'b0;
    assign axisr_bypass_tx_req_to_switch.tdata  = '0;
    assign axisr_bypass_tx_req_to_switch.tkeep  = '0;
    assign axisr_bypass_tx_req_to_switch.tlast  = 1'b0;
    assign axisr_bypass_tx_req_to_switch.tid    = '0;
    assign axisr_bypass_tx_req_to_switch.tdest  = '0;

    // Bypass TX RSP: unused for bypass (bypass doesn't have separate REQ/RSP like RDMA)
    assign axisr_bypass_tx_rsp_to_switch.tvalid = 1'b0;
    assign axisr_bypass_tx_rsp_to_switch.tdata  = '0;
    assign axisr_bypass_tx_rsp_to_switch.tkeep  = '0;
    assign axisr_bypass_tx_rsp_to_switch.tlast  = 1'b0;
    assign axisr_bypass_tx_rsp_to_switch.tid    = '0;
    assign axisr_bypass_tx_rsp_to_switch.tdest  = '0;
    assign axisr_bypass_tx_rsp_from_switch.tready = 1'b1;

    // Bypass RX from switch: unused (RX data goes to vFIU via axisr_vfiu_from_switch)
    assign axisr_bypass_rx_from_switch.tready = 1'b1;
{% else %}
    // Bypass disabled - tie off all bypass vIO Switch interfaces
    assign axisr_bypass_tx_req_to_switch.tvalid = 1'b0;
    assign axisr_bypass_tx_req_to_switch.tdata  = '0;
    assign axisr_bypass_tx_req_to_switch.tkeep  = '0;
    assign axisr_bypass_tx_req_to_switch.tlast  = 1'b0;
    assign axisr_bypass_tx_req_to_switch.tid    = '0;
    assign axisr_bypass_tx_req_to_switch.tdest  = '0;
    assign axisr_bypass_tx_req_from_switch.tready = 1'b1;

    assign axisr_bypass_tx_rsp_to_switch.tvalid = 1'b0;
    assign axisr_bypass_tx_rsp_to_switch.tdata  = '0;
    assign axisr_bypass_tx_rsp_to_switch.tkeep  = '0;
    assign axisr_bypass_tx_rsp_to_switch.tlast  = 1'b0;
    assign axisr_bypass_tx_rsp_to_switch.tid    = '0;
    assign axisr_bypass_tx_rsp_to_switch.tdest  = '0;
    assign axisr_bypass_tx_rsp_from_switch.tready = 1'b1;

    assign axisr_bypass_rx_from_switch.tready = 1'b1;

    assign axisr_bypass_rx_to_switch.tvalid = 1'b0;
    assign axisr_bypass_rx_to_switch.tdata  = '0;
    assign axisr_bypass_rx_to_switch.tkeep  = '0;
    assign axisr_bypass_rx_to_switch.tlast  = 1'b0;
    assign axisr_bypass_rx_to_switch.tid    = '0;
    assign axisr_bypass_rx_to_switch.tdest  = '0;
{% endif %}

    // Instantiate vIO Switch (full crossbar for data routing)
    vio_switch_{{ cnfg.n_reg }} inst_vio_switch (
        .aclk(aclk),
        .aresetn(aresetn),

        // Route signals for vFIUs
        .route_in(vfiu_route_out),
        .route_out(vfiu_route_in),

        // Host interfaces (unused for RDMA test)
        .data_host_tx_sink(axisr_host_tx_to_switch),
        .data_host_tx_src(axisr_host_tx_from_switch),
        .data_host_rx_sink(axisr_host_rx_to_switch),
        .data_host_rx_src(axisr_host_rx_from_switch),

        // vFIU interfaces (per region)
        .data_vfiu_sink(axisr_vfiu_to_switch),
        .data_vfiu_src(axisr_vfiu_from_switch),

        // RDMA interfaces
        .data_rdma_rx_sink(axisr_rdma_rx_to_switch),
        .data_rdma_rx_src(axisr_rdma_rx_from_switch),
        .data_rdma_tx_req_sink(axisr_rdma_tx_req_to_switch),
        .data_rdma_tx_req_src(axisr_rdma_tx_req_from_switch),
        .data_rdma_tx_rsp_sink(axisr_rdma_tx_rsp_to_switch),
        .data_rdma_tx_rsp_src(axisr_rdma_tx_rsp_from_switch),

        // TCP interface - POS: RX path goes through vIO Switch for vFIU routing
        .data_tcp_sink(axisr_tcp_to_switch),   // TCP RX from arbiter to vIO Switch
        .data_tcp_src(axisr_tcp_from_switch),  // Unused (TX bypasses vIO Switch)

        // Bypass interfaces (unused for RDMA test)
        .data_bypass_rx_sink(axisr_bypass_rx_to_switch),
        .data_bypass_rx_src(axisr_bypass_rx_from_switch),
        .data_bypass_tx_req_sink(axisr_bypass_tx_req_to_switch),
        .data_bypass_tx_req_src(axisr_bypass_tx_req_from_switch),
        .data_bypass_tx_rsp_sink(axisr_bypass_tx_rsp_to_switch),
        .data_bypass_tx_rsp_src(axisr_bypass_tx_rsp_from_switch)
    );

    // ========================================================================================
    // vFIU Instantiation (per region)
    // ========================================================================================
    // Each vFPGA region has its own vFIU (vFPGA Isolation Unit) that handles:
    //   - RX Demux: Routes incoming vIO Switch data to correct protocol path
    //   - TX Mux: Combines 5 TX paths into single stream for vIO Switch
    //   - gateway_send: Attaches route_id to all 5 TX paths
    //   - gateway_recv: Validates incoming routes on all 5 RX paths
    //   - gate_mem: Validates memory requests against configured endpoints
    //
    // vfiu_top has 2 AXI4SR ports to vIO Switch (matching vio_switch_N.sv):
    //   - s_axis_switch: RX from vIO Switch (data_vfiu_src[i])
    //   - m_axis_switch: TX to vIO Switch (data_vfiu_sink[i])
    // ========================================================================================

    // ----------------------------------------------------------------------------------------
    // Memory Endpoint Control (placeholder - will be configured by host)
    // ----------------------------------------------------------------------------------------
    logic [N_REGIONS-1:0][(99*4)-1:0] vfiu_mem_ctrl;  // 4 endpoints per vFIU
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_mem_ctrl_init
        assign vfiu_mem_ctrl[i] = '0;  // Default: no memory restrictions
    end

    // Per-vFIU memory request interfaces
    // Input: from bpss_rd/wr_sq (user bypass requests after ccross/slice/decoupler)
    // Output: validated requests to mmu_top
    metaIntf #(.STYPE(req_t)) vfiu_rd_req [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) vfiu_wr_req [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) vfiu_rd_req_out [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) vfiu_wr_req_out [N_REGIONS] ();

    // Connect user bypass requests to gate_mem input (validated by vfiu_top)
    // Flow: user → ccross → slice → decoupler → bpss_rd/wr_sq → gate_mem → bpss_rd/wr_sq_valid → mmu_top
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_vfiu_mem_req
        // Connect bpss requests to gate_mem input
        assign vfiu_rd_req[i].valid = bpss_rd_sq[i].valid;
        assign vfiu_rd_req[i].data  = bpss_rd_sq[i].data;
        assign bpss_rd_sq[i].ready  = vfiu_rd_req[i].ready;

        assign vfiu_wr_req[i].valid = bpss_wr_sq[i].valid;
        assign vfiu_wr_req[i].data  = bpss_wr_sq[i].data;
        assign bpss_wr_sq[i].ready  = vfiu_wr_req[i].ready;

        // Connect gate_mem output to validated request interfaces (go to mmu_top)
        assign bpss_rd_sq_valid[i].valid = vfiu_rd_req_out[i].valid;
        assign bpss_rd_sq_valid[i].data  = vfiu_rd_req_out[i].data;
        assign vfiu_rd_req_out[i].ready  = bpss_rd_sq_valid[i].ready;

        assign bpss_wr_sq_valid[i].valid = vfiu_wr_req_out[i].valid;
        assign bpss_wr_sq_valid[i].data  = vfiu_wr_req_out[i].data;
        assign vfiu_wr_req_out[i].ready  = bpss_wr_sq_valid[i].ready;
    end

    // ----------------------------------------------------------------------------------------
    // RDMA TX tie-offs when disabled (REQ and RSP kept separate to vfiu_top)
    // ----------------------------------------------------------------------------------------
{% if not cnfg.en_rdma %}
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tie_off_rdma_req [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tie_off_rdma_rsp [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_tieoff_rx [N_REGIONS] ();
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_rdma_tieoff
        assign axis_tie_off_rdma_req[i].tvalid = 1'b0;
        assign axis_tie_off_rdma_req[i].tdata  = '0;
        assign axis_tie_off_rdma_req[i].tkeep  = '0;
        assign axis_tie_off_rdma_req[i].tlast  = 1'b0;
        assign axis_tie_off_rdma_rsp[i].tvalid = 1'b0;
        assign axis_tie_off_rdma_rsp[i].tdata  = '0;
        assign axis_tie_off_rdma_rsp[i].tkeep  = '0;
        assign axis_tie_off_rdma_rsp[i].tlast  = 1'b0;
        assign axis_rdma_tieoff_rx[i].tready = 1'b1;
    end
{% endif %}

    // ----------------------------------------------------------------------------------------
    // Protocol tie-offs when disabled
    // ----------------------------------------------------------------------------------------
{% if not cnfg.en_tcp %}
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tie_off_tcp [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_tieoff_rx [N_REGIONS] ();
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_tcp_tieoff
        assign axis_tie_off_tcp[i].tvalid = 1'b0;
        assign axis_tie_off_tcp[i].tdata  = '0;
        assign axis_tie_off_tcp[i].tkeep  = '0;
        assign axis_tie_off_tcp[i].tlast  = 1'b0;
        assign axis_tcp_tieoff_rx[i].tready = 1'b1;
    end
{% endif %}

{% if not cnfg.en_bypass %}
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tie_off_bypass [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_tieoff_rx [N_REGIONS] ();
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_bypass_tieoff
        assign axis_tie_off_bypass[i].tvalid = 1'b0;
        assign axis_tie_off_bypass[i].tdata  = '0;
        assign axis_tie_off_bypass[i].tkeep  = '0;
        assign axis_tie_off_bypass[i].tlast  = 1'b0;
        assign axis_bypass_tieoff_rx[i].tready = 1'b1;
    end
{% endif %}

    // ----------------------------------------------------------------------------------------
    // vfiu_top Instantiation (per region)
    // ----------------------------------------------------------------------------------------
    // vfiu_top now handles demux/mux internally - clean 2-port interface to vIO Switch
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_vfiu_top

        vfiu_top #(
            .N_REGIONS(N_REGIONS),
            .N_ENDPOINTS(4),
            .VFPGA_ID(i)
        ) inst_vfiu_top (
            .aclk(aclk),
            .aresetn(aresetn),

            // vIO Switch interface (2 unidirectional ports)
            .s_axis_switch(axisr_vfiu_from_switch[i]),  // RX: vIO Switch src → vFIU
            .m_axis_switch(axisr_vfiu_to_switch[i]),    // TX: vFIU → vIO Switch sink

            // Memory endpoint control
            .mem_ctrl(vfiu_mem_ctrl[i]),

            // Memory request interfaces (gate_mem)
            .s_rd_req(vfiu_rd_req[i]),
            .s_wr_req(vfiu_wr_req[i]),
            .m_rd_req(vfiu_rd_req_out[i]),
            .m_wr_req(vfiu_wr_req_out[i]),

            // Routing control
            .route_ctrl(vfiu_route_ctrl[i]),

            // HOST Path (AXI4S - no tid needed, PID comes from local_credits)
            .s_axis_host_tx(axis_host_out_s1[i]),
            .m_axis_host_rx(axis_host_in_s1[i]),

            // RDMA Path (AXI4S) - REQ and RSP kept separate to vfiu_top
{% if cnfg.en_rdma %}
            .s_axis_rdma_tx_req(axis_rdma_out_req_s1[i]),
            .s_axis_rdma_tx_rsp(axis_rdma_out_rsp_s1[i]),
            .m_axis_rdma_rx(axis_rdma_in_s1[i]),
{% else %}
            .s_axis_rdma_tx_req(axis_tie_off_rdma_req[i]),
            .s_axis_rdma_tx_rsp(axis_tie_off_rdma_rsp[i]),
            .m_axis_rdma_rx(axis_rdma_tieoff_rx[i]),
{% endif %}

            // TCP Path (AXI4S)
{% if cnfg.en_tcp %}
            .s_axis_tcp_tx(axis_tcp_out_s1[i]),
            .m_axis_tcp_rx(axis_tcp_in_s1[i]),
{% else %}
            .s_axis_tcp_tx(axis_tie_off_tcp[i]),
            .m_axis_tcp_rx(axis_tcp_tieoff_rx[i]),
{% endif %}

            // BYPASS Path (AXI4S)
{% if cnfg.en_bypass %}
            .s_axis_bypass_tx(axis_bypass_out_s1[i]),
            .m_axis_bypass_rx(axis_bypass_in_s1[i]),
{% else %}
            .s_axis_bypass_tx(axis_tie_off_bypass[i]),
            .m_axis_bypass_rx(axis_bypass_tieoff_rx[i]),
{% endif %}

            // P2P Path (AXI4SR - tid = sender vFPGA ID)
            .s_axis_p2p_tx(axisr_p2p_out[i]),
            .m_axis_p2p_rx(axisr_p2p_in[i])
        );

    end // gen_vfiu_top

    // NOTE: vfiu_top handles the TX mux and RX demux internally.
    // The m_axis_switch port from vfiu_top is directly connected to axisr_vfiu_to_switch[i].
    // gateway_send and gateway_recv are also instantiated inside vfiu_top.
    //
    // HOST path uses AXI4S (no TID) - PID comes from local_credits in user_wrapper.

    // Extract route_out from vfiu_top TX for vIO Switch routing
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_vfiu_route_out
        assign vfiu_route_out[i] = axisr_vfiu_to_switch[i].tdest;
    end

    // Clock crossing (if enabled)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) rdma_sq_s2 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) rdma_cq_s2 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_rd_s2 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_wr_s2 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_in_s2 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_req_s2 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_rsp_s2 [N_REGIONS] ();

{% if cnfg.en_uclk %}
    // Cross to UCLK
    for(genvar i = 0; i < N_REGIONS; i++) begin
        rdma_ccross_ul (
            .nclk(aclk), // Used for net crossing (same here)
            .nresetn(aresetn),
            .aclk(uclk),
            .aresetn(uresetn),

            .m_rdma_sq_nclk(rdma_sq_s1[i]),
            .s_rdma_ack_nclk(rdma_cq_s1[i]),
            .s_rdma_rd_req_nclk(rdma_rq_rd_s1[i]),
            .s_rdma_wr_req_nclk(rdma_rq_wr_s1[i]),
            .m_axis_rdma_rd_req_nclk(axis_rdma_out_req_s1[i]),
            .m_axis_rdma_rd_rsp_nclk(axis_rdma_out_rsp_s1[i]),
            .s_axis_rdma_wr_nclk(axis_rdma_in_s1[i]),

            .s_rdma_sq_aclk(rdma_sq_s2[i]),
            .m_rdma_ack_aclk(rdma_cq_s2[i]),
            .m_rdma_rd_req_aclk(rdma_rq_rd_s2[i]),
            .m_rdma_wr_req_aclk(rdma_rq_wr_s2[i]),
            .s_axis_rdma_rd_req_aclk(axis_rdma_out_req_s2[i]),
            .s_axis_rdma_rd_rsp_aclk(axis_rdma_out_rsp_s2[i]),
            .m_axis_rdma_wr_aclk(axis_rdma_in_s2[i])
        );
    end

{% else %}
    // No crossing
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `META_ASSIGN(rdma_sq_s2[i], rdma_sq_s1[i])
        `META_ASSIGN(rdma_cq_s1[i], rdma_cq_s2[i])
        `META_ASSIGN(rdma_rq_rd_s1[i], rdma_rq_rd_s2[i])
        `META_ASSIGN(rdma_rq_wr_s1[i], rdma_rq_wr_s2[i])
        `AXIS_ASSIGN(axis_rdma_in_s1[i], axis_rdma_in_s2[i])
        `AXIS_ASSIGN(axis_rdma_out_req_s2[i], axis_rdma_out_req_s1[i])
        `AXIS_ASSIGN(axis_rdma_out_rsp_s2[i], axis_rdma_out_rsp_s1[i])
    end

{% endif %}

    // Slice 1
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) rdma_sq_s3 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) rdma_cq_s3 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_rd_s3 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_wr_s3 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_in_s3 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_req_s3 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_rsp_s3 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        rdma_slice_array_ul #(
            .N_STAGES(N_REG_DYN_NET_S1)
        ) inst_rdma_slice_array_1 (
{% if cnfg.en_uclk %}
            .aclk(uclk),
            .aresetn(uresetn),
{% else %}
            .aclk(aclk),
            .aresetn(aresetn),
{% endif %}
            .m_rdma_sq_n(rdma_sq_s2[i]),
            .s_rdma_ack_n(rdma_cq_s2[i]),
            .s_rdma_rd_req_n(rdma_rq_rd_s2[i]),
            .s_rdma_wr_req_n(rdma_rq_wr_s2[i]),
            .m_axis_rdma_rd_req_n(axis_rdma_out_req_s2[i]),
            .m_axis_rdma_rd_rsp_n(axis_rdma_out_rsp_s2[i]),
            .s_axis_rdma_wr_n(axis_rdma_in_s2[i]),

            .s_rdma_sq_u(rdma_sq_s3[i]),
            .m_rdma_ack_u(rdma_cq_s3[i]),
            .m_rdma_rd_req_u(rdma_rq_rd_s3[i]),
            .m_rdma_wr_req_u(rdma_rq_wr_s3[i]),
            .s_axis_rdma_rd_req_u(axis_rdma_out_req_s3[i]),
            .s_axis_rdma_rd_rsp_u(axis_rdma_out_rsp_s3[i]),
            .m_axis_rdma_wr_u(axis_rdma_in_s3[i])
        );
    end
    
    // Decoupling 
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) rdma_sq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) rdma_cq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_rd_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) rdma_rq_wr_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_in_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_req_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_rdma_out_rsp_ul [N_REGIONS] ();

    meta_decoupler (.decouple(decouple_uclk), .s_meta(rdma_sq_ul), .m_meta(rdma_sq_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(rdma_cq_s3), .m_meta(rdma_cq_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(rdma_rq_rd_s3), .m_meta(rdma_rq_rd_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(rdma_rq_wr_s3), .m_meta(rdma_rq_wr_ul));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_rdma_in_s3), .m_axis(axis_rdma_in_ul));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_rdma_out_req_ul), .m_axis(axis_rdma_out_req_s3));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_rdma_out_rsp_ul), .m_axis(axis_rdma_out_rsp_s3));

{% endif %}
{% if cnfg.en_tcp %}
    // ================-----------------------------------------------------------------
    // TCP/IP
    // ================-----------------------------------------------------------------
    
    // Slice 0
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(tcp_listen_req_t)) tcp_listen_req_s0();
    metaIntf #(.STYPE(tcp_listen_rsp_t)) tcp_listen_rsp_s0();
    metaIntf #(.STYPE(tcp_open_req_t)) tcp_open_req_s0();
    metaIntf #(.STYPE(tcp_open_rsp_t)) tcp_open_rsp_s0();
    metaIntf #(.STYPE(tcp_close_req_t)) tcp_close_req_s0 ();
    metaIntf #(.STYPE(tcp_notify_t)) tcp_notify_s0 ();
    metaIntf #(.STYPE(tcp_rd_pkg_t)) tcp_rd_pkg_s0 ();
    metaIntf #(.STYPE(tcp_rx_meta_t)) tcp_rx_meta_s0 ();
    metaIntf #(.STYPE(tcp_tx_meta_t)) tcp_tx_meta_s0 ();
    metaIntf #(.STYPE(tcp_tx_stat_t)) tcp_tx_stat_s0 ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_in_s0 ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_out_s0 ();
    
    tcp_slice_array_net #(
        .N_STAGES(N_REG_DYN_NET_S0)
    ) inst_tcp_slice_array (
        .aclk(aclk),
        .aresetn(aresetn),

        .m_tcp_listen_req_n(m_tcp_listen_req),
        .s_tcp_listen_rsp_n(s_tcp_listen_rsp),
        .m_tcp_open_req_n(m_tcp_open_req),
        .s_tcp_open_rsp_n(s_tcp_open_rsp),
        .m_tcp_close_req_n(m_tcp_close_req),
        .s_tcp_notify_n(s_tcp_notify),
        .m_tcp_rd_pkg_n(m_tcp_rd_pkg),
        .s_tcp_rx_meta_n(s_tcp_rx_meta),
        .m_tcp_tx_meta_n(m_tcp_tx_meta),
        .s_tcp_tx_stat_n(s_tcp_tx_stat),
        .s_axis_tcp_rx_n(s_axis_tcp),
        .m_axis_tcp_tx_n(m_axis_tcp),

        .s_tcp_listen_req_u(tcp_listen_req_s0),
        .m_tcp_listen_rsp_u(tcp_listen_rsp_s0),
        .s_tcp_open_req_u(tcp_open_req_s0),
        .m_tcp_open_rsp_u(tcp_open_rsp_s0),
        .s_tcp_close_req_u(tcp_close_req_s0),
        .m_tcp_notify_u(tcp_notify_s0),
        .s_tcp_rd_pkg_u(tcp_rd_pkg_s0),
        .m_tcp_rx_meta_u(tcp_rx_meta_s0),
        .s_tcp_tx_meta_u(tcp_tx_meta_s0),
        .m_tcp_tx_stat_u(tcp_tx_stat_s0),
        .m_axis_tcp_rx_u(axis_tcp_in_s0),
        .s_axis_tcp_tx_u(axis_tcp_out_s0)
    );		

    // Arbitration
    // ----------------------------------------------------------------------
    // Arbitrate METADATA only (data routing via vIO Switch)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(tcp_listen_req_t)) tcp_listen_req_s1[N_REGIONS]();
    metaIntf #(.STYPE(tcp_listen_rsp_t)) tcp_listen_rsp_s1[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_req_t)) tcp_open_req_s1[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_rsp_t)) tcp_open_rsp_s1[N_REGIONS]();
    metaIntf #(.STYPE(tcp_close_req_t)) tcp_close_req_s1 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_notify_t)) tcp_notify_s1 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rd_pkg_t)) tcp_rd_pkg_s1 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rx_meta_t)) tcp_rx_meta_s1 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_meta_t)) tcp_tx_meta_s1 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_stat_t)) tcp_tx_stat_s1 [N_REGIONS]();

    // POS: Per-region TCP data interfaces (routed through vIO Switch)
    // TCP RX data comes from vIO Switch (vFIU output), TCP TX goes to vIO Switch (vFIU input)
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_in_s1 [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_out_s1 [N_REGIONS]();

    // vIO Switch routing signals from TCP metadata arbiter (full route_id format)
    // Route format: [13:10]=reserved, [9:6]=sender_id, [5:2]=receiver_id, [1:0]=flags
    logic [13:0] tcp_route_id_rx;
    logic [13:0] tcp_route_id_tx;

    tcp_meta_only_arbiter inst_tcp_meta_arbiter (
        .aclk(aclk),
        .aresetn(aresetn),

        // Network side - metadata only
        .m_tcp_listen_req_net(tcp_listen_req_s0),
        .s_tcp_listen_rsp_net(tcp_listen_rsp_s0),
        .m_tcp_open_req_net(tcp_open_req_s0),
        .s_tcp_open_rsp_net(tcp_open_rsp_s0),
        .m_tcp_close_req_net(tcp_close_req_s0),
        .s_tcp_notify_net(tcp_notify_s0),
        .m_tcp_rd_pkg_net(tcp_rd_pkg_s0),
        .s_tcp_rx_meta_net(tcp_rx_meta_s0),
        .m_tcp_tx_meta_net(tcp_tx_meta_s0),
        .s_tcp_tx_stat_net(tcp_tx_stat_s0),

        // User side - metadata only (per region)
        .s_tcp_listen_req_user(tcp_listen_req_s1),
        .m_tcp_listen_rsp_user(tcp_listen_rsp_s1),
        .s_tcp_open_req_user(tcp_open_req_s1),
        .m_tcp_open_rsp_user(tcp_open_rsp_s1),
        .s_tcp_close_req_user(tcp_close_req_s1),
        .m_tcp_notify_user(tcp_notify_s1),
        .s_tcp_rd_pkg_user(tcp_rd_pkg_s1),
        .m_tcp_rx_meta_user(tcp_rx_meta_s1),
        .s_tcp_tx_meta_user(tcp_tx_meta_s1),
        .m_tcp_tx_stat_user(tcp_tx_stat_s1),

        // vIO Switch routing (full route_id with sender_id encoded)
        .route_id_rx(tcp_route_id_rx),
        .route_id_tx(tcp_route_id_tx)
    );

    // ============================================================================
    // POS vIO Switch TCP Data Routing
    // ============================================================================
    // TCP data flows through vIO Switch just like RDMA:
    //   TCP RX (network->vFPGA): axis_tcp_in_s0 -> vIO Switch TCP sink -> vFIU[vfid] src -> user
    //   TCP TX (vFPGA->network): user -> vFIU[i] sink -> vIO Switch TCP src -> axis_tcp_out_s0
    //
    // The vIO Switch routes based on tdest. TCP shares vFIU ports with RDMA.
    // ----------------------------------------------------------------------

    // ============================================================================
    // TCP RX Path: Network -> vIO Switch TCP sink -> vFIU -> User
    // ============================================================================
    // Convert AXI4S (from TCP stack) to AXI4SR (for vIO Switch) for RX
    // tdest = route_id_rx from metadata arbiter (includes sender_id=PORT_TCP)
    // Route format: [13:10]=reserved, [9:6]=sender_id, [5:2]=receiver_id, [1:0]=flags
    // For RX from network: sender=PORT_TCP, receiver=vfid (destination vFPGA)
    assign axisr_tcp_to_switch.tvalid = axis_tcp_in_s0.tvalid;
    assign axisr_tcp_to_switch.tdata  = axis_tcp_in_s0.tdata;
    assign axisr_tcp_to_switch.tkeep  = axis_tcp_in_s0.tkeep;
    assign axisr_tcp_to_switch.tlast  = axis_tcp_in_s0.tlast;
    assign axisr_tcp_to_switch.tid    = '0;
    // tdest = full route_id with sender_id=PORT_TCP, receiver_id=vfid
    assign axisr_tcp_to_switch.tdest  = tcp_route_id_rx;
    assign axis_tcp_in_s0.tready      = axisr_tcp_to_switch.tready;

    // ============================================================================
    // TCP TX Path: User -> vFIU -> vIO Switch TCP src -> Network
    // ============================================================================
    // Convert AXI4SR (from vIO Switch TCP src) to AXI4S (to TCP stack) for TX
    assign axis_tcp_out_s0.tvalid       = axisr_tcp_from_switch.tvalid;
    assign axis_tcp_out_s0.tdata        = axisr_tcp_from_switch.tdata;
    assign axis_tcp_out_s0.tkeep        = axisr_tcp_from_switch.tkeep;
    assign axis_tcp_out_s0.tlast        = axisr_tcp_from_switch.tlast;
    assign axisr_tcp_from_switch.tready = axis_tcp_out_s0.tready;

    // Extract route_id from vIO Switch TCP output for VIU VLAN tagging
    logic [13:0] tcp_tx_route_out;
    assign tcp_tx_route_out = axisr_tcp_from_switch.tdest;

    // POS: TCP route_id outputs for VIU
    assign m_tcp_tx_route_id       = tcp_tx_route_out;
    assign m_tcp_tx_route_id_valid = axisr_tcp_from_switch.tvalid;

    // Clock crossing (if enabled)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(tcp_listen_req_t)) tcp_listen_req_s2[N_REGIONS]();
    metaIntf #(.STYPE(tcp_listen_rsp_t)) tcp_listen_rsp_s2[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_req_t)) tcp_open_req_s2[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_rsp_t)) tcp_open_rsp_s2[N_REGIONS]();
    metaIntf #(.STYPE(tcp_close_req_t)) tcp_close_req_s2 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_notify_t)) tcp_notify_s2 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rd_pkg_t)) tcp_rd_pkg_s2 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rx_meta_t)) tcp_rx_meta_s2 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_meta_t)) tcp_tx_meta_s2 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_stat_t)) tcp_tx_stat_s2 [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_in_s2 [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_out_s2 [N_REGIONS]();

{% if cnfg.en_uclk %}
    // Cross to UCLK
    for(genvar i = 0; i < N_REGIONS; i++) begin
        tcp_ccross_ul inst_tcp_ccross_ul (
            .nclk(aclk), // Used for net crossing (same here)
            .nresetn(aresetn),
            .aclk(uclk),
            .aresetn(uresetn),

            .m_tcp_listen_req_nclk(tcp_listen_req_s1[i]),
            .s_tcp_listen_rsp_nclk(tcp_listen_rsp_s1[i]),
            .m_tcp_open_req_nclk(tcp_open_req_s1[i]),
            .s_tcp_open_rsp_nclk(tcp_open_rsp_s1[i]),
            .m_tcp_close_req_nclk(tcp_close_req_s1[i]),
            .s_tcp_notify_nclk(tcp_notify_s1[i]),
            .m_tcp_rd_pkg_nclk(tcp_rd_pkg_s1[i]),
            .s_tcp_rx_meta_nclk(tcp_rx_meta_s1[i]),
            .m_tcp_tx_meta_nclk(tcp_tx_meta_s1[i]),
            .s_tcp_tx_stat_nclk(tcp_tx_stat_s1[i]),
            .s_axis_tcp_rx_nclk(axis_tcp_in_s1[i]),
            .m_axis_tcp_tx_nclk(axis_tcp_out_s1[i]),


            .s_tcp_listen_req_aclk(tcp_listen_req_s2[i]),
            .m_tcp_listen_rsp_aclk(tcp_listen_rsp_s2[i]),
            .s_tcp_open_req_aclk(tcp_open_req_s2[i]),
            .m_tcp_open_rsp_aclk(tcp_open_rsp_s2[i]),
            .s_tcp_close_req_aclk(tcp_close_req_s2[i]),
            .m_tcp_notify_aclk(tcp_notify_s2[i]),
            .s_tcp_rd_pkg_aclk(tcp_rd_pkg_s2[i]),
            .m_tcp_rx_meta_aclk(tcp_rx_meta_s2[i]),
            .s_tcp_tx_meta_aclk(tcp_tx_meta_s2[i]),
            .m_tcp_tx_stat_aclk(tcp_tx_stat_s2[i]),
            .m_axis_tcp_rx_aclk(axis_tcp_in_s2[i]),
            .s_axis_tcp_tx_aclk(axis_tcp_out_s2[i])
        );
    end

{% else %}
    // No crossing
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `META_ASSIGN(tcp_listen_req_s2[i], tcp_listen_req_s1[i])
        `META_ASSIGN(tcp_listen_rsp_s1[i], tcp_listen_rsp_s2[i])
        `META_ASSIGN(tcp_open_req_s2[i], tcp_open_req_s1[i])
        `META_ASSIGN(tcp_open_rsp_s1[i], tcp_open_rsp_s2[i])
        `META_ASSIGN(tcp_close_req_s2[i], tcp_close_req_s1[i])        
        `META_ASSIGN(tcp_notify_s1[i], tcp_notify_s2[i])
        `META_ASSIGN(tcp_rd_pkg_s2[i], tcp_rd_pkg_s1[i])
        `META_ASSIGN(tcp_rx_meta_s1[i], tcp_rx_meta_s2[i])
        `META_ASSIGN(tcp_tx_meta_s2[i], tcp_tx_meta_s1[i])
        `META_ASSIGN(tcp_tx_stat_s1[i], tcp_tx_stat_s2[i])
        `AXIS_ASSIGN(axis_tcp_in_s1[i], axis_tcp_in_s2[i])
        `AXIS_ASSIGN(axis_tcp_out_s2[i], axis_tcp_out_s1[i])
    end

{% endif %}	

    // Slice 1
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(tcp_listen_req_t)) tcp_listen_req_s3[N_REGIONS]();
    metaIntf #(.STYPE(tcp_listen_rsp_t)) tcp_listen_rsp_s3[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_req_t)) tcp_open_req_s3[N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_rsp_t)) tcp_open_rsp_s3[N_REGIONS]();
    metaIntf #(.STYPE(tcp_close_req_t)) tcp_close_req_s3 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_notify_t)) tcp_notify_s3 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rd_pkg_t)) tcp_rd_pkg_s3 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rx_meta_t)) tcp_rx_meta_s3 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_meta_t)) tcp_tx_meta_s3 [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_stat_t)) tcp_tx_stat_s3 [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_in_s3 [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_out_s3 [N_REGIONS]();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        tcp_slice_array_ul #(
            .N_STAGES(N_REG_DYN_NET_S1)
        ) inst_tcp_slice_array_ul (
{% if cnfg.en_uclk %}
            .aclk(uclk),
            .aresetn(uresetn),
{% else %}
            .aclk(aclk),
            .aresetn(aresetn),
{% endif %}
            
            .m_tcp_listen_req_n(tcp_listen_req_s2[i]),
            .s_tcp_listen_rsp_n(tcp_listen_rsp_s2[i]),
            .m_tcp_open_req_n(tcp_open_req_s2[i]),
            .s_tcp_open_rsp_n(tcp_open_rsp_s2[i]),
            .m_tcp_close_req_n(tcp_close_req_s2[i]),
            .s_tcp_notify_n(tcp_notify_s2[i]),
            .m_tcp_rd_pkg_n(tcp_rd_pkg_s2[i]),
            .s_tcp_rx_meta_n(tcp_rx_meta_s2[i]),
            .m_tcp_tx_meta_n(tcp_tx_meta_s2[i]),
            .s_tcp_tx_stat_n(tcp_tx_stat_s2[i]),
            .s_axis_tcp_rx_n(axis_tcp_in_s2[i]),
            .m_axis_tcp_tx_n(axis_tcp_out_s2[i]),

            .s_tcp_listen_req_u(tcp_listen_req_s3[i]),
            .m_tcp_listen_rsp_u(tcp_listen_rsp_s3[i]),
            .s_tcp_open_req_u(tcp_open_req_s3[i]),
            .m_tcp_open_rsp_u(tcp_open_rsp_s3[i]),
            .s_tcp_close_req_u(tcp_close_req_s3[i]),
            .m_tcp_notify_u(tcp_notify_s3[i]),
            .s_tcp_rd_pkg_u(tcp_rd_pkg_s3[i]),
            .m_tcp_rx_meta_u(tcp_rx_meta_s3[i]),
            .s_tcp_tx_meta_u(tcp_tx_meta_s3[i]),
            .m_tcp_tx_stat_u(tcp_tx_stat_s3[i]),
            .m_axis_tcp_rx_u(axis_tcp_in_s3[i]),
            .s_axis_tcp_tx_u(axis_tcp_out_s3[i])
        );	
    end
    
    // Decoupling 
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(tcp_listen_req_t)) tcp_listen_req_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_listen_rsp_t)) tcp_listen_rsp_ul     [N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_req_t)) tcp_open_req_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_open_rsp_t)) tcp_open_rsp_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_close_req_t)) tcp_close_req_ul [N_REGIONS]();

    metaIntf #(.STYPE(tcp_notify_t)) tcp_notify_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rd_pkg_t)) tcp_rd_pkg_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_rx_meta_t)) tcp_rx_meta_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_meta_t)) tcp_tx_meta_ul [N_REGIONS]();
    metaIntf #(.STYPE(tcp_tx_stat_t)) tcp_tx_stat_ul [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_in_ul [N_REGIONS]();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_tcp_out_ul [N_REGIONS]();
    
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_listen_req_ul), .m_meta(tcp_listen_req_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_listen_rsp_s3), .m_meta(tcp_listen_rsp_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_open_req_ul),   .m_meta(tcp_open_req_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_open_rsp_s3),   .m_meta(tcp_open_rsp_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_close_req_ul),  .m_meta(tcp_close_req_s3));

    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_notify_s3),       .m_meta(tcp_notify_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_rd_pkg_ul),     .m_meta(tcp_rd_pkg_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_rx_meta_s3),      .m_meta(tcp_rx_meta_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_tx_meta_ul),    .m_meta(tcp_tx_meta_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(tcp_tx_stat_s3),      .m_meta(tcp_tx_stat_ul));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_tcp_in_s3),    .m_axis(axis_tcp_in_ul));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_tcp_out_ul), .m_axis(axis_tcp_out_s3));

{% endif %}

{% if cnfg.en_bypass %}
    // ================-----------------------------------------------------------------
    // BYPASS - Pipeline Stages (ccross, slice, decoupler)
    // ================-----------------------------------------------------------------
    // Bypass path needs same pipeline stages as RDMA/TCP for proper isolation.
    // Uses bypass_ccross_ul and bypass_slice_array_ul which bundle control AND data:
    //   s1 (from arbiter/vIO Switch) -> s2 (ccross) -> s3 (slice) -> ul (decouple) -> user
    //
    // Control signals:
    //   bypass_sq:     TX send queue (user -> network)
    //   bypass_rq_rd:  RX read request (network -> user)
    //   bypass_rq_wr:  RX write request (network -> user)
    //
    // Data signals:
    //   axis_bypass_rd_rsp (axis_bypass_out): TX read response data (user -> network)
    //   axis_bypass_wr (axis_bypass_in):      RX write data (network -> user)

    // Clock crossing stage (s1 -> s2)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) bypass_sq_s2 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_rd_s2 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_wr_s2 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_rd_rsp_s2 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_wr_s2 [N_REGIONS] ();

{% if cnfg.en_uclk %}
    // Cross to UCLK
    for(genvar i = 0; i < N_REGIONS; i++) begin
        bypass_ccross_ul inst_bypass_ccross (
            .nclk(aclk),
            .nresetn(aresetn),
            .aclk(uclk),
            .aresetn(uresetn),

            // Network clock side (s1)
            .m_bypass_sq_nclk(bypass_sq_s1[i]),
            .s_bypass_rd_req_nclk(bypass_rq_rd_s1[i]),
            .s_bypass_wr_req_nclk(bypass_rq_wr_s1[i]),
            .m_axis_bypass_rd_rsp_nclk(axis_bypass_out_s1[i]),
            .s_axis_bypass_wr_nclk(axis_bypass_in_s1[i]),

            // User clock side (s2)
            .s_bypass_sq_aclk(bypass_sq_s2[i]),
            .m_bypass_rd_req_aclk(bypass_rq_rd_s2[i]),
            .m_bypass_wr_req_aclk(bypass_rq_wr_s2[i]),
            .s_axis_bypass_rd_rsp_aclk(axis_bypass_rd_rsp_s2[i]),
            .m_axis_bypass_wr_aclk(axis_bypass_wr_s2[i])
        );
    end

{% else %}
    // No crossing
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `META_ASSIGN(bypass_sq_s2[i], bypass_sq_s1[i])
        `META_ASSIGN(bypass_rq_rd_s1[i], bypass_rq_rd_s2[i])
        `META_ASSIGN(bypass_rq_wr_s1[i], bypass_rq_wr_s2[i])
        `AXIS_ASSIGN(axis_bypass_rd_rsp_s2[i], axis_bypass_out_s1[i])
        `AXIS_ASSIGN(axis_bypass_in_s1[i], axis_bypass_wr_s2[i])
    end

{% endif %}

    // Slice stage (s2 -> s3)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) bypass_sq_s3 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_rd_s3 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_wr_s3 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_rd_rsp_s3 [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_wr_s3 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        bypass_slice_array_ul #(
            .N_STAGES(N_REG_DYN_NET_S1)
        ) inst_bypass_slice_array (
{% if cnfg.en_uclk %}
            .aclk(uclk),
            .aresetn(uresetn),
{% else %}
            .aclk(aclk),
            .aresetn(aresetn),
{% endif %}
            // Network side (s2)
            .m_bypass_sq_n(bypass_sq_s2[i]),
            .s_bypass_rd_req_n(bypass_rq_rd_s2[i]),
            .s_bypass_wr_req_n(bypass_rq_wr_s2[i]),
            .m_axis_bypass_rd_rsp_n(axis_bypass_rd_rsp_s2[i]),
            .s_axis_bypass_wr_n(axis_bypass_wr_s2[i]),

            // User side (s3)
            .s_bypass_sq_u(bypass_sq_s3[i]),
            .m_bypass_rd_req_u(bypass_rq_rd_s3[i]),
            .m_bypass_wr_req_u(bypass_rq_wr_s3[i]),
            .s_axis_bypass_rd_rsp_u(axis_bypass_rd_rsp_s3[i]),
            .m_axis_bypass_wr_u(axis_bypass_wr_s3[i])
        );
    end

    // Decoupling stage (s3 -> ul)
    // ----------------------------------------------------------------------
    metaIntf #(.STYPE(dreq_t)) bypass_sq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_rd_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bypass_rq_wr_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_rd_rsp_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_wr_ul [N_REGIONS] ();

    meta_decoupler (.decouple(decouple_uclk), .s_meta(bypass_sq_ul), .m_meta(bypass_sq_s3));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bypass_rq_rd_s3), .m_meta(bypass_rq_rd_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bypass_rq_wr_s3), .m_meta(bypass_rq_wr_ul));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_bypass_rd_rsp_ul), .m_axis(axis_bypass_rd_rsp_s3));
    axis_decoupler (.decouple(decouple_uclk), .s_axis(axis_bypass_wr_s3), .m_axis(axis_bypass_wr_ul));

    // Bypass data signal aliases for user_wrapper compatibility
    // axis_bypass_in_ul  = RX write data from network (axis_bypass_wr_ul)
    // axis_bypass_out_ul = TX read response to network (axis_bypass_rd_rsp_ul)
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_in_ul [N_REGIONS] ();
    AXI4S #(.AXI4S_DATA_BITS(AXI_NET_BITS)) axis_bypass_out_ul [N_REGIONS] ();

    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_bypass_ul_aliases
        // RX: Write data from network to user
        `AXIS_ASSIGN(axis_bypass_wr_ul[i], axis_bypass_in_ul[i])
        // TX: Read response from user to network
        `AXIS_ASSIGN(axis_bypass_out_ul[i], axis_bypass_rd_rsp_ul[i])
    end
{% endif %}

    // P2P - Direct connection using AXI4SR with TID (like microShell)
    // No ccross/slice needed - same clock domain within shell, direct path to user
    // P2P is vFPGA-to-vFPGA communication routed through vio_switch.
    // RX: vio_switch → gateway_recv (validation) → axisr_p2p_in → user (tid = sender vFPGA)
    // TX: user → axisr_p2p_out → gateway_send (validation + route_id) → vio_switch
{% if cnfg.en_p2p %}
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_p2p_ul
        // RX: Direct from gateway_recv output to user (AXI4SR with tid)
        `AXISR_ASSIGN(axisr_p2p_in[i], axisr_p2p_in_ul[i])
        // TX: Direct from user to gateway_send input (AXI4SR with tid)
        `AXISR_ASSIGN(axisr_p2p_out_ul[i], axisr_p2p_out[i])
    end
{% else %}
    // P2P not enabled - tie off interfaces (AXI4SR)
    for (genvar i = 0; i < N_REGIONS; i++) begin : gen_p2p_tieoff
        // RX: Discard incoming P2P data
        assign axisr_p2p_in[i].tready = 1'b1;
        // TX: No P2P data to send
        assign axisr_p2p_out[i].tvalid = 1'b0;
        assign axisr_p2p_out[i].tdata  = '0;
        assign axisr_p2p_out[i].tkeep  = '0;
        assign axisr_p2p_out[i].tlast  = 1'b0;
        assign axisr_p2p_out[i].tid    = '0;
    end
{% endif %}

    // ================-----------------------------------------------------------------
	// Rest of interfaces
	// ================-----------------------------------------------------------------

    // Control lTLB
	AXI4L axi_ctrl_lTlb [N_REGIONS] ();
	
	// Control sTLB
	AXI4L axi_ctrl_sTlb [N_REGIONS] ();
	
    // Control config
    AXI4L axi_ctrl_cnfg [N_REGIONS] ();
    
    // Control user logic
    AXI4L axi_ctrl_user [N_REGIONS] ();
    
    // Slice 0 
    // ----------------------------------------------------------------------
    AXI4L axi_ctrl_s0 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        axil_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) (.aclk(aclk), .aresetn(aresetn), .s_axi(s_axi_ctrl[i]), .m_axi(axi_ctrl_s0[i]));
    end

{% if cnfg.en_avx %}
    AXI4 #(.AXI4_DATA_BITS(AVX_DATA_BITS)) axim_ctrl_s0 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
        axim_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) (.aclk(aclk), .aresetn(aresetn), .s_axi(s_axim_ctrl[i]), .m_axi(axim_ctrl_s0[i]));
    end

{% endif %}

    // Clock crossing (if enabled)
    // ----------------------------------------------------------------------
    AXI4L axi_ctrl_user_s0 [N_REGIONS] ();
    metaIntf #(.STYPE(irq_not_t)) notify [N_REGIONS] ();
    metaIntf #(.STYPE(irq_not_t)) notify_s0 [N_REGIONS] ();
    metaIntf #(.STYPE(dreq_t)) host_sq [N_REGIONS] ();
    metaIntf #(.STYPE(dreq_t)) host_sq_s0 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_rd_sq [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_wr_sq [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_rd_sq_s0 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_wr_sq_s0 [N_REGIONS] ();
    // Validated bypass requests (after gate_mem validation in vfiu_top)
    metaIntf #(.STYPE(req_t)) bpss_rd_sq_valid [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_wr_sq_valid [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_rd_cq [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_wr_cq [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_rd_cq_s0 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_wr_cq_s0 [N_REGIONS] ();


{% if cnfg.en_uclk %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        axil_ccross (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_axi(axi_ctrl_user[i]), .m_axi(axi_ctrl_user_s0[i]));
        meta_ccross #(.DATA_BITS(NOTIFY_BITS)) (.s_aclk(uclk), .s_aresetn(uresetn), .m_aclk(aclk), .m_aresetn(aresetn), .s_meta(notify_s0[i]), .m_meta(notify[i]));
        meta_ccross #(.DATA_BITS($bits(dreq_t))) (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_meta(host_sq[i]), .m_meta(host_sq_s0[i]));
        meta_ccross #(.DATA_BITS($bits(req_t))) (.s_aclk(uclk), .s_aresetn(uresetn), .m_aclk(aclk), .m_aresetn(aresetn), .s_meta(bpss_rd_sq_s0[i]), .m_meta(bpss_rd_sq[i]));
        meta_ccross #(.DATA_BITS($bits(req_t))) (.s_aclk(uclk), .s_aresetn(uresetn), .m_aclk(aclk), .m_aresetn(aresetn), .s_meta(bpss_wr_sq_s0[i]), .m_meta(bpss_wr_sq[i]));
        meta_ccross #(.DATA_BITS($bits(ack_t))) (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_meta(bpss_rd_cq[i]), .m_meta(bpss_rd_cq_s0[i]));
        meta_ccross #(.DATA_BITS($bits(ack_t))) (.s_aclk(aclk), .s_aresetn(aresetn), .m_aclk(uclk), .m_aresetn(uresetn), .s_meta(bpss_wr_cq[i]), .m_meta(bpss_wr_cq_s0[i]));
        
    end

{% else %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `AXIL_ASSIGN(axi_ctrl_user[i], axi_ctrl_user_s0[i])
        `META_ASSIGN(notify_s0[i], notify[i])
        `META_ASSIGN(host_sq[i], host_sq_s0[i])
        `META_ASSIGN(bpss_rd_sq_s0[i], bpss_rd_sq[i])
        `META_ASSIGN(bpss_wr_sq_s0[i], bpss_wr_sq[i])
        `META_ASSIGN(bpss_rd_cq[i], bpss_rd_cq_s0[i])
        `META_ASSIGN(bpss_wr_cq[i], bpss_wr_cq_s0[i])
    end

{% endif %}	

    // Slice 1
	// ----------------------------------------------------------------------
    AXI4L axi_ctrl_user_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(irq_not_t)) notify_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(dreq_t)) host_sq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_rd_sq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_wr_sq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_rd_cq_s1 [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_wr_cq_s1 [N_REGIONS] ();

    for(genvar i = 0; i < N_REGIONS; i++) begin
{% if cnfg.en_uclk %}
        axil_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) (.aclk(uclk), .aresetn(uresetn), .s_axi(axi_ctrl_user_s0[i]), .m_axi(axi_ctrl_user_s1[i]));
        meta_reg_array #(.DATA_BITS(NOTIFY_BITS), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(notify_s1[i]), .m_meta(notify_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(dreq_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(host_sq_s0[i]), .m_meta(host_sq_s1[i]));
        meta_reg_array #(.DATA_BITS($bits(req_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(bpss_rd_sq_s1[i]), .m_meta(bpss_rd_sq_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(req_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(bpss_wr_sq_s1[i]), .m_meta(bpss_wr_sq_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(ack_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(bpss_rd_cq_s0[i]), .m_meta(bpss_rd_cq_s1[i]));
        meta_reg_array #(.DATA_BITS($bits(ack_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(uclk), .aresetn(uresetn), .s_meta(bpss_wr_cq_s0[i]), .m_meta(bpss_wr_cq_s1[i]));
{% else %}
        axil_reg_array #(.N_STAGES(N_REG_DYN_HOST_S0)) (.aclk(aclk), .aresetn(aresetn), .s_axi(axi_ctrl_user_s0[i]), .m_axi(axi_ctrl_user_s1[i]));
        meta_reg_array #(.DATA_BITS(NOTIFY_BITS), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(notify_s1[i]), .m_meta(notify_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(dreq_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(host_sq_s0[i]), .m_meta(host_sq_s1[i]));
        meta_reg_array #(.DATA_BITS($bits(req_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(bpss_rd_sq_s1[i]), .m_meta(bpss_rd_sq_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(req_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(bpss_wr_sq_s1[i]), .m_meta(bpss_wr_sq_s0[i]));
        meta_reg_array #(.DATA_BITS($bits(ack_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(bpss_rd_cq_s0[i]), .m_meta(bpss_rd_cq_s1[i]));
        meta_reg_array #(.DATA_BITS($bits(ack_t)), .N_STAGES(N_REG_DYN_HOST_S1)) (.aclk(aclk), .aresetn(aresetn), .s_meta(bpss_wr_cq_s0[i]), .m_meta(bpss_wr_cq_s1[i]));
{% endif %}
    end	

	// Decoupling 
	// ----------------------------------------------------------------------
	AXI4L axi_ctrl_user_ul [N_REGIONS] ();
    metaIntf #(.STYPE(irq_not_t)) notify_ul [N_REGIONS] ();
    metaIntf #(.STYPE(dreq_t)) host_sq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_rd_sq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(req_t)) bpss_wr_sq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_rd_cq_ul [N_REGIONS] ();
    metaIntf #(.STYPE(ack_t)) bpss_wr_cq_ul [N_REGIONS] ();
    

    axil_decoupler (.decouple(decouple_uclk), .s_axi(axi_ctrl_user_s1), .m_axi(axi_ctrl_user_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(notify_ul), .m_meta(notify_s1));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(host_sq_s1), .m_meta(host_sq_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bpss_rd_sq_ul), .m_meta(bpss_rd_sq_s1));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bpss_wr_sq_ul), .m_meta(bpss_wr_sq_s1));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bpss_rd_cq_s1), .m_meta(bpss_rd_cq_ul));
    meta_decoupler (.decouple(decouple_uclk), .s_meta(bpss_wr_cq_s1), .m_meta(bpss_wr_cq_ul));

    // ================-----------------------------------------------------------------
	// MMU and CONFIG 
	// ================-----------------------------------------------------------------
    
    dynamic_crossbar #(
        .ID_DYN(ID_DYN)
	) inst_dyn_crossbar (
        .aclk(aclk),
        .aresetn(aresetn),
        .s_axi_ctrl(axi_ctrl_s0),
        .m_axi_ctrl_cnfg(axi_ctrl_cnfg),
        .m_axi_ctrl_sTlb(axi_ctrl_sTlb),
        .m_axi_ctrl_lTlb(axi_ctrl_lTlb),
        .m_axi_ctrl_user(axi_ctrl_user)
	);
	
{% if cnfg.en_avx %}
    for(genvar i = 0; i < N_REGIONS; i++) begin
        `AXIL_TIE_OFF_S(axi_ctrl_cnfg[i])
    end

{% endif %}	
    // Dynamic management
	mmu_top #(
        .ID_DYN(ID_DYN)
	) inst_mmu_top (
        .aclk(aclk),
        .aresetn(aresetn),
        .s_axi_ctrl_lTlb(axi_ctrl_lTlb),
        .s_axi_ctrl_sTlb(axi_ctrl_sTlb),
{% if cnfg.en_avx %}
		.s_axim_ctrl_cnfg(axim_ctrl_s0),
{% else %}
		.s_axi_ctrl_cnfg(axi_ctrl_cnfg),
{% endif %}	
        .s_notify(notify),
        .m_host_sq(host_sq),
        // Use validated bypass requests (after gate_mem in vfiu_top)
        .s_bpss_rd_sq(bpss_rd_sq_valid),
        .s_bpss_wr_sq(bpss_wr_sq_valid),
        .m_bpss_rd_cq(bpss_rd_cq),
        .m_bpss_wr_cq(bpss_wr_cq),
{% if cnfg.en_strm %}
        .m_rd_XDMA_host(rd_XDMA_host),
        .m_wr_XDMA_host(wr_XDMA_host),
        .m_mux_host_rd(mux_host_rd_user),
        .m_mux_host_wr(mux_host_wr_user),
{% if cnfg.en_cred_local == 0 %}
        .rxfer_host(rxfer_host),
        .wxfer_host(wxfer_host),
{% endif %}
{% endif %}	
{% if cnfg.en_mem %}
        .m_rd_XDMA_mig(rd_XDMA_mig),
        .m_wr_XDMA_mig(wr_XDMA_mig),
        .m_rd_CDMA_mig(rd_CDMA_mig),
        .m_wr_CDMA_mig(wr_CDMA_mig),
        .m_rd_CDMA_card(rd_CDMA_card),
        .m_wr_CDMA_card(wr_CDMA_card),
{% if cnfg.en_cred_local == 0 %}
        .rxfer_card(rxfer_card),
        .wxfer_card(wxfer_card),
{% endif %}
{% endif %}	
{% if cnfg.en_net %}
        .m_arp_lookup_request(arp_lup_req_s0),
{% endif %}
{% if cnfg.en_rdma %}
        .m_rdma_qp_interface(rdma_qp_interface_s0),
        .m_rdma_conn_interface(rdma_conn_interface_s0),
        .s_rdma_cq(rdma_host_cq_s1),
{% endif %}
{% if cnfg.en_wb %}
        .m_wback(m_wback),
{% endif %}	
        .usr_irq(m_usr_irq)
	);
	
	// ================-----------------------------------------------------------------
	// USER 
	// ================-----------------------------------------------------------------
{% for i in range(0, cnfg.n_reg) %}
    design_user_wrapper_{{ i }} inst_user_wrapper_{{ i }} ( 
        
        // AXIL CTRL
        .axi_ctrl_araddr                (axi_ctrl_user_ul[{{ i }}].araddr),
        .axi_ctrl_arprot                (axi_ctrl_user_ul[{{ i }}].arprot),
        .axi_ctrl_arready               (axi_ctrl_user_ul[{{ i }}].arready),
        .axi_ctrl_arvalid               (axi_ctrl_user_ul[{{ i }}].arvalid),
        .axi_ctrl_awaddr                (axi_ctrl_user_ul[{{ i }}].awaddr),
        .axi_ctrl_awprot                (axi_ctrl_user_ul[{{ i }}].awprot),
        .axi_ctrl_awready               (axi_ctrl_user_ul[{{ i }}].awready),
        .axi_ctrl_awvalid               (axi_ctrl_user_ul[{{ i }}].awvalid),
        .axi_ctrl_bready                (axi_ctrl_user_ul[{{ i }}].bready),
        .axi_ctrl_bresp                 (axi_ctrl_user_ul[{{ i }}].bresp),
        .axi_ctrl_bvalid                (axi_ctrl_user_ul[{{ i }}].bvalid),
        .axi_ctrl_rdata                 (axi_ctrl_user_ul[{{ i }}].rdata),
        .axi_ctrl_rready                (axi_ctrl_user_ul[{{ i }}].rready),
        .axi_ctrl_rresp                 (axi_ctrl_user_ul[{{ i }}].rresp),
        .axi_ctrl_rvalid                (axi_ctrl_user_ul[{{ i }}].rvalid),
        .axi_ctrl_wdata                 (axi_ctrl_user_ul[{{ i }}].wdata),
        .axi_ctrl_wready                (axi_ctrl_user_ul[{{ i }}].wready),
        .axi_ctrl_wstrb                 (axi_ctrl_user_ul[{{ i }}].wstrb),
        .axi_ctrl_wvalid                (axi_ctrl_user_ul[{{ i }}].wvalid),

        // NOTIFY
        .notify_valid                   (notify_ul[{{ i }}].valid),
        .notify_ready                   (notify_ul[{{ i }}].ready),
        .notify_data                    (notify_ul[{{ i }}].data),

        // HOST DESC
        .host_sq_valid	                (host_sq_ul[{{ i }}].valid),
        .host_sq_ready	                (host_sq_ul[{{ i }}].ready),
        .host_sq_data	                (host_sq_ul[{{ i }}].data),

        // BPSS DESC
        .bpss_rd_sq_valid	            (bpss_rd_sq_ul[{{ i }}].valid),
        .bpss_rd_sq_ready	            (bpss_rd_sq_ul[{{ i }}].ready),
        .bpss_rd_sq_data	            (bpss_rd_sq_ul[{{ i }}].data),
        .bpss_wr_sq_valid	            (bpss_wr_sq_ul[{{ i }}].valid),
        .bpss_wr_sq_ready	            (bpss_wr_sq_ul[{{ i }}].ready),
        .bpss_wr_sq_data	            (bpss_wr_sq_ul[{{ i }}].data),
        .bpss_rd_cq_valid               (bpss_rd_cq_ul[{{ i }}].valid),
        .bpss_rd_cq_ready               (bpss_rd_cq_ul[{{ i }}].ready),
        .bpss_rd_cq_data                (bpss_rd_cq_ul[{{ i }}].data),
        .bpss_wr_cq_valid               (bpss_wr_cq_ul[{{ i }}].valid),
        .bpss_wr_cq_ready               (bpss_wr_cq_ul[{{ i }}].ready),
        .bpss_wr_cq_data                (bpss_wr_cq_ul[{{ i }}].data),
        
{% if cnfg.en_strm %}
        // HOST STREAMS (AXI4S - PID comes from local_credits, not sideband TID)
        .axis_host_sink_tdata           (axis_host_in_ul[{{ i }}].tdata),
        .axis_host_sink_tkeep           (axis_host_in_ul[{{ i }}].tkeep),
        .axis_host_sink_tlast           (axis_host_in_ul[{{ i }}].tlast),
        .axis_host_sink_tready          (axis_host_in_ul[{{ i }}].tready),
        .axis_host_sink_tvalid          (axis_host_in_ul[{{ i }}].tvalid),
        .axis_host_src_tdata            (axis_host_out_ul[{{ i }}].tdata),
        .axis_host_src_tkeep            (axis_host_out_ul[{{ i }}].tkeep),
        .axis_host_src_tlast            (axis_host_out_ul[{{ i }}].tlast),
        .axis_host_src_tready           (axis_host_out_ul[{{ i }}].tready),
        .axis_host_src_tvalid           (axis_host_out_ul[{{ i }}].tvalid),
{% endif %}
{% if cnfg.en_mem %}
        // CARD STREAMS
    {% for j in range(0, cnfg.n_card_axi) %}
        .axis_card_{{j}}_sink_tdata     (axis_card_in_ul[{{i}}*N_CARD_AXI+{{j}}].tdata),
        .axis_card_{{j}}_sink_tkeep     (axis_card_in_ul[{{i}}*N_CARD_AXI+{{j}}].tkeep),
        .axis_card_{{j}}_sink_tlast     (axis_card_in_ul[{{i}}*N_CARD_AXI+{{j}}].tlast),
        .axis_card_{{j}}_sink_tready    (axis_card_in_ul[{{i}}*N_CARD_AXI+{{j}}].tready),
        .axis_card_{{j}}_sink_tvalid    (axis_card_in_ul[{{i}}*N_CARD_AXI+{{j}}].tvalid),
        .axis_card_{{j}}_src_tdata      (axis_card_out_ul[{{i}}*N_CARD_AXI+{{j}}].tdata),
        .axis_card_{{j}}_src_tkeep      (axis_card_out_ul[{{i}}*N_CARD_AXI+{{j}}].tkeep),
        .axis_card_{{j}}_src_tlast      (axis_card_out_ul[{{i}}*N_CARD_AXI+{{j}}].tlast),
        .axis_card_{{j}}_src_tready     (axis_card_out_ul[{{i}}*N_CARD_AXI+{{j}}].tready),
        .axis_card_{{j}}_src_tvalid     (axis_card_out_ul[{{i}}*N_CARD_AXI+{{j}}].tvalid),
    {% endfor %}
{% endif %}
{% if cnfg.en_rdma %}
        // RDMA DESC
        .rdma_sq_valid                  (rdma_sq_ul[{{ i }}].valid),
        .rdma_sq_ready                  (rdma_sq_ul[{{ i }}].ready),
        .rdma_sq_data                   (rdma_sq_ul[{{ i }}].data),
        .rdma_cq_valid                  (rdma_cq_ul[{{ i }}].valid),
        .rdma_cq_ready                  (rdma_cq_ul[{{ i }}].ready),
        .rdma_cq_data                   (rdma_cq_ul[{{ i }}].data), 
        .rdma_rq_rd_valid               (rdma_rq_rd_ul[{{ i }}].valid),
        .rdma_rq_rd_ready               (rdma_rq_rd_ul[{{ i }}].ready),
        .rdma_rq_rd_data                (rdma_rq_rd_ul[{{ i }}].data), 
        .rdma_rq_wr_valid               (rdma_rq_wr_ul[{{ i }}].valid),
        .rdma_rq_wr_ready               (rdma_rq_wr_ul[{{ i }}].ready),
        .rdma_rq_wr_data                (rdma_rq_wr_ul[{{ i }}].data), 

        // RDMA STREAMS
        .axis_rdma_sink_tdata           (axis_rdma_in_ul[{{ i }}].tdata),
        .axis_rdma_sink_tkeep           (axis_rdma_in_ul[{{ i }}].tkeep),
        .axis_rdma_sink_tlast           (axis_rdma_in_ul[{{ i }}].tlast),
        .axis_rdma_sink_tready          (axis_rdma_in_ul[{{ i }}].tready),
        .axis_rdma_sink_tvalid          (axis_rdma_in_ul[{{ i }}].tvalid),
        .axis_rdma_src_req_tdata            (axis_rdma_out_req_ul[{{ i }}].tdata),
        .axis_rdma_src_req_tkeep            (axis_rdma_out_req_ul[{{ i }}].tkeep),
        .axis_rdma_src_req_tlast            (axis_rdma_out_req_ul[{{ i }}].tlast),
        .axis_rdma_src_req_tready           (axis_rdma_out_req_ul[{{ i }}].tready),
        .axis_rdma_src_req_tvalid           (axis_rdma_out_req_ul[{{ i }}].tvalid),
        .axis_rdma_src_rsp_tdata            (axis_rdma_out_rsp_ul[{{ i }}].tdata),
        .axis_rdma_src_rsp_tkeep            (axis_rdma_out_rsp_ul[{{ i }}].tkeep),
        .axis_rdma_src_rsp_tlast            (axis_rdma_out_rsp_ul[{{ i }}].tlast),
        .axis_rdma_src_rsp_tready           (axis_rdma_out_rsp_ul[{{ i }}].tready),
        .axis_rdma_src_rsp_tvalid           (axis_rdma_out_rsp_ul[{{ i }}].tvalid),
{% endif %}
{% if cnfg.en_tcp %}
   // TCP DESC
        .tcp_listen_req_valid	(tcp_listen_req_ul[{{ i }}].valid),
        .tcp_listen_req_ready	(tcp_listen_req_ul[{{ i }}].ready),
        .tcp_listen_req_data    (tcp_listen_req_ul[{{ i }}].data),
        .tcp_listen_rsp_valid   (tcp_listen_rsp_ul[{{ i }}].valid),
        .tcp_listen_rsp_ready   (tcp_listen_rsp_ul[{{ i }}].ready),
        .tcp_listen_rsp_data    (tcp_listen_rsp_ul[{{ i }}].data),
        .tcp_open_req_valid	    (tcp_open_req_ul[{{ i }}].valid),
        .tcp_open_req_ready	    (tcp_open_req_ul[{{ i }}].ready),
        .tcp_open_req_data	    (tcp_open_req_ul[{{ i }}].data),
        .tcp_open_rsp_valid	    (tcp_open_rsp_ul[{{ i }}].valid),
        .tcp_open_rsp_ready	    (tcp_open_rsp_ul[{{ i }}].ready),
        .tcp_open_rsp_data  	(tcp_open_rsp_ul[{{ i }}].data),
        .tcp_close_req_valid	(tcp_close_req_ul[{{ i }}].valid),
        .tcp_close_req_ready	(tcp_close_req_ul[{{ i }}].ready),
        .tcp_close_req_data	    (tcp_close_req_ul[{{ i }}].data),  

        .tcp_notify_valid		(tcp_notify_ul[{{ i }}].valid),
        .tcp_notify_ready		(tcp_notify_ul[{{ i }}].ready),
        .tcp_notify_data		(tcp_notify_ul[{{ i }}].data),
        .tcp_rd_pkg_valid	    (tcp_rd_pkg_ul[{{ i }}].valid),
        .tcp_rd_pkg_ready	    (tcp_rd_pkg_ul[{{ i }}].ready),
        .tcp_rd_pkg_data	    (tcp_rd_pkg_ul[{{ i }}].data),
        .tcp_rx_meta_valid	    (tcp_rx_meta_ul[{{ i }}].valid),
        .tcp_rx_meta_ready	    (tcp_rx_meta_ul[{{ i }}].ready),
        .tcp_rx_meta_data		(tcp_rx_meta_ul[{{ i }}].data),
        .tcp_tx_meta_valid	    (tcp_tx_meta_ul[{{ i }}].valid),
        .tcp_tx_meta_ready	    (tcp_tx_meta_ul[{{ i }}].ready),
        .tcp_tx_meta_data		(tcp_tx_meta_ul[{{ i }}].data),
        .tcp_tx_stat_valid	    (tcp_tx_stat_ul[{{ i }}].valid),
        .tcp_tx_stat_ready	    (tcp_tx_stat_ul[{{ i }}].ready),
        .tcp_tx_stat_data		(tcp_tx_stat_ul[{{ i }}].data),        

        .axis_tcp_sink_tdata            (axis_tcp_in_ul[{{ i }}].tdata),
        .axis_tcp_sink_tkeep            (axis_tcp_in_ul[{{ i }}].tkeep),
        .axis_tcp_sink_tlast            (axis_tcp_in_ul[{{ i }}].tlast),
        .axis_tcp_sink_tready           (axis_tcp_in_ul[{{ i }}].tready),
        .axis_tcp_sink_tvalid           (axis_tcp_in_ul[{{ i }}].tvalid),
        .axis_tcp_src_tdata             (axis_tcp_out_ul[{{ i }}].tdata),
        .axis_tcp_src_tkeep             (axis_tcp_out_ul[{{ i }}].tkeep),
        .axis_tcp_src_tlast             (axis_tcp_out_ul[{{ i }}].tlast),
        .axis_tcp_src_tready            (axis_tcp_out_ul[{{ i }}].tready),
        .axis_tcp_src_tvalid            (axis_tcp_out_ul[{{ i }}].tvalid),
{% endif %}
{% if cnfg.en_bypass %}
        // BYPASS CONTROL (SQ and RQ commands)
        .bypass_sq_valid                (bypass_sq_ul[{{ i }}].valid),
        .bypass_sq_ready                (bypass_sq_ul[{{ i }}].ready),
        .bypass_sq_data                 (bypass_sq_ul[{{ i }}].data),
        .bypass_rq_rd_valid             (bypass_rq_rd_ul[{{ i }}].valid),
        .bypass_rq_rd_ready             (bypass_rq_rd_ul[{{ i }}].ready),
        .bypass_rq_rd_data              (bypass_rq_rd_ul[{{ i }}].data),
        .bypass_rq_wr_valid             (bypass_rq_wr_ul[{{ i }}].valid),
        .bypass_rq_wr_ready             (bypass_rq_wr_ul[{{ i }}].ready),
        .bypass_rq_wr_data              (bypass_rq_wr_ul[{{ i }}].data),

        // BYPASS STREAMS
        .axis_bypass_sink_tdata         (axis_bypass_in_ul[{{ i }}].tdata),
        .axis_bypass_sink_tkeep         (axis_bypass_in_ul[{{ i }}].tkeep),
        .axis_bypass_sink_tlast         (axis_bypass_in_ul[{{ i }}].tlast),
        .axis_bypass_sink_tready        (axis_bypass_in_ul[{{ i }}].tready),
        .axis_bypass_sink_tvalid        (axis_bypass_in_ul[{{ i }}].tvalid),
        .axis_bypass_src_tdata          (axis_bypass_out_ul[{{ i }}].tdata),
        .axis_bypass_src_tkeep          (axis_bypass_out_ul[{{ i }}].tkeep),
        .axis_bypass_src_tlast          (axis_bypass_out_ul[{{ i }}].tlast),
        .axis_bypass_src_tready         (axis_bypass_out_ul[{{ i }}].tready),
        .axis_bypass_src_tvalid         (axis_bypass_out_ul[{{ i }}].tvalid),
{% endif %}
{% if cnfg.en_p2p %}
        // P2P STREAMS (vFPGA-to-vFPGA communication) - AXI4SR with TID (like microShell)
        // RX: tid = sender vFPGA ID
        .axis_p2p_sink_tdata            (axisr_p2p_in_ul[{{ i }}].tdata),
        .axis_p2p_sink_tkeep            (axisr_p2p_in_ul[{{ i }}].tkeep),
        .axis_p2p_sink_tlast            (axisr_p2p_in_ul[{{ i }}].tlast),
        .axis_p2p_sink_tready           (axisr_p2p_in_ul[{{ i }}].tready),
        .axis_p2p_sink_tvalid           (axisr_p2p_in_ul[{{ i }}].tvalid),
        .axis_p2p_sink_tid              (axisr_p2p_in_ul[{{ i }}].tid),
        // TX: tid for user identification
        .axis_p2p_src_tdata             (axisr_p2p_out_ul[{{ i }}].tdata),
        .axis_p2p_src_tkeep             (axisr_p2p_out_ul[{{ i }}].tkeep),
        .axis_p2p_src_tlast             (axisr_p2p_out_ul[{{ i }}].tlast),
        .axis_p2p_src_tready            (axisr_p2p_out_ul[{{ i }}].tready),
        .axis_p2p_src_tvalid            (axisr_p2p_out_ul[{{ i }}].tvalid),
        .axis_p2p_src_tid               (axisr_p2p_out_ul[{{ i }}].tid),
{% endif %}

        .S_BSCAN_drck(S_BSCAN_drck[{{ i }}]),
        .S_BSCAN_shift(S_BSCAN_shift[{{ i }}]),
        .S_BSCAN_tdi(S_BSCAN_tdi[{{ i }}]),
        .S_BSCAN_update(S_BSCAN_update[{{ i }}]),
        .S_BSCAN_sel(S_BSCAN_sel[{{ i }}]),
        .S_BSCAN_tdo(S_BSCAN_tdo[{{ i }}]),
        .S_BSCAN_tms(S_BSCAN_tms[{{ i }}]),
        .S_BSCAN_tck(S_BSCAN_tck[{{ i }}]),
        .S_BSCAN_runtest(S_BSCAN_runtest[{{ i }}]),
        .S_BSCAN_reset(S_BSCAN_reset[{{ i }}]),
        .S_BSCAN_capture(S_BSCAN_capture[{{ i }}]),
        .S_BSCAN_bscanid_en(S_BSCAN_bscanid_en[{{ i }}]),
{% if cnfg.en_uclk %}
        .aclk                   (uclk),
        .aresetn                (uresetn),
{% else %}
        .aclk                   (aclk),
        .aresetn                (aresetn),
{% endif %}
        .dclk                   (dclk)
    );

{% endfor %}
	
endmodule
